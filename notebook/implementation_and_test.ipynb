{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import easydict\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_dim: int, \n",
    "                 num_heads: int, \n",
    "                 drop_out: float = 0.0,\n",
    "                 bias: bool = False, \n",
    "                 encoder_decoder_attention: bool = False,\n",
    "                 causal: bool = False):\n",
    "        '''Initialize MultiHeadAttention class variables.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim (int): Dimension of a word * number of heads.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            drop_out (float): Drop out rate.\n",
    "            bias (bool): Boolean that indicating whether to use bias or not.\n",
    "            encoder_decoder_attention (bool): Boolean that indicating whether the multi head\n",
    "                                              attention is encoder-decoder attention or not.\n",
    "            causal (bool): Boolean that indicating whether to use causal mask or not.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        assert self.head_dim * num_heads == emb_dim, \"emb_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.drop_out = drop_out\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        self.causal = causal\n",
    "        \n",
    "        self.wk = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.wq = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.wv = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.output = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "    \n",
    "    def multi_head_scaled_dot_product(self,\n",
    "                                      query: torch.Tensor,\n",
    "                                      key: torch.Tensor,\n",
    "                                      value: torch.Tensor,\n",
    "                                      attention_mask: torch.BoolTensor):\n",
    "        '''Perform multi-head version of scaled dot product.\n",
    "        \n",
    "        Args:\n",
    "            query (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            key (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            value (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            attention_mask: This mask can be either causal mask or padding mask.\n",
    "                            shape '(batch size, source squence length)' for padding mask.\n",
    "                            shape '(sequence length, target sequence length)' for causal mask.\n",
    "        Returns:\n",
    "            attn_output (Tensor): output of attention mechanism. shape '(batch size, seq_len, emb_dim)'\n",
    "            attn_weights (Tensor): value of attention weight of each word. shape '(batch size, # attn head, seq_len, seq_len)'\n",
    "        '''\n",
    "        \n",
    "        attn_weights = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "        '''shape of attn_weights : (batch size, # attn head, seq_len, seq_len)'''\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            if self.causal:\n",
    "                '''Masking future info for encoder-decoder attention.'''\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n",
    "                '''\n",
    "                shape of attention_mask : (trg_len, trg_len).\n",
    "                shape of attention_mask.unsqueeze(0).unsqueeze(1) : (1, 1, trg_len, trg_len).\n",
    "                '''\n",
    "            else:\n",
    "                '''Masking padding token so that it is not used for attention.'''\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "                '''\n",
    "                shape of attention_mask : (batch_size, src_len)\n",
    "                shape of attention_mask.unsqueeze(1).unsqueeze(2) : (batch_size, 1, 1, src_len)\n",
    "                '''\n",
    "        attn_weights = F.softmax(attn_weights, dim = -1)\n",
    "        attn_probs = F.dropout(attn_weights, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_probs, value)\n",
    "        '''shape of attn_output : (batch size, # attn head, seq_len, head_dim)'''\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
    "        '''shape of attn_output : (batch size, seq_len, # attn head, head_dim)'''\n",
    "        shape = attn_output.size()[:-2] + (self.emb_dim,)\n",
    "        attn_output = attn_output.view(*shape)\n",
    "        '''shape of attn_output : (batch size, seq_len, emb_dim)'''\n",
    "        attn_output = self.output(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def transform_to_multi_head(self, \n",
    "                                x: torch.Tensor):\n",
    "        ''' Reshape input\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: shape '(batch_size, # attn head, seq_len, head_dim)'\n",
    "        '''\n",
    "        \n",
    "        shape = x.size()[:-1] + (self.num_heads, self.head_dim,)\n",
    "        x = x.view(*shape)\n",
    "        \n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                attention_mask: torch.Tensor = None):\n",
    "        '''\n",
    "        Args:\n",
    "            query (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "            key (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "            attention_mask (Tensor): shape '(batch size, squence length)' for padding mask.\n",
    "                                     shape '(sequence length, sequence length)' for causal mask.\n",
    "        \n",
    "        Returns:\n",
    "            attn_output (Tensor): output of attention mechanism. shape '(batch size, seq_len, emb_dim)'\n",
    "            attn_weights (Tensor): value of attention weight of each word. shape '(batch size, # attn head, seq_l\n",
    "        '''\n",
    "        \n",
    "        q = self.wq(query)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        if self.encoder_decoder_attention:\n",
    "            '''\n",
    "            query is output of encoder\n",
    "            key is input of decoder\n",
    "            '''\n",
    "            k = self.wk(key)\n",
    "            v = self.wv(key)\n",
    "        \n",
    "        # self attention\n",
    "        else:\n",
    "            '''\n",
    "            Both of query and key are input of encoder(query is same with key).\n",
    "            '''\n",
    "            k = self.wk(query)\n",
    "            v = self.wv(query)\n",
    "        \n",
    "        q = self.transform_to_multi_head(q)\n",
    "        k = self.transform_to_multi_head(k)\n",
    "        v = self.transform_to_multi_head(v)\n",
    "        \n",
    "        attn_output, attn_weights = self.multi_head_scaled_dot_product(q,k,v,attention_mask)\n",
    "            \n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 45, 512]) torch.Size([3, 8, 45, 45])\n"
     ]
    }
   ],
   "source": [
    "# temp_mha = MultiHeadAttention(emb_dim=512, num_heads=8)\n",
    "# x = torch.rand(3, 45, 512)\n",
    "# out, attn = temp_mha(query=x, key=x, attention_mask=None)\n",
    "# print(out.size(), attn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb_dim: int,\n",
    "                 hid_dim: int,\n",
    "                 drop_out: float = 0.1):\n",
    "        '''Initialize position-wise feed forward network.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim (int): word embdding dimension.\n",
    "            hid_dim (int): hidden dimesion.\n",
    "            drop_out (float): drop out rate.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(emb_dim, hid_dim)\n",
    "        self.linear_2 = nn.Linear(hid_dim, emb_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "    \n",
    "    def forward(self,\n",
    "                 x: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        \n",
    "        Return:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        '''\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        x = self.linear_2(x)\n",
    "        x = F.dropout(x, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 45, 512])\n"
     ]
    }
   ],
   "source": [
    "# temp_pwff = PositionWiseFeedForward(512, 256)\n",
    "# x = torch.rand(3, 45, 512)\n",
    "# out = temp_pwff(x)\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncodedEmbedding(nn.Embedding):\n",
    "    def __init__(self, \n",
    "                 max_position: int, \n",
    "                 embedding_dim: int):\n",
    "        '''Initialize positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            max_position (int): maximum length of input sequence length.\n",
    "                                 That is, it can encode position up to max_positions'th position.\n",
    "            embedding_dim (int): embedding dimension.\n",
    "        '''\n",
    "        super().__init__(max_position, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    def _init_weight(self, initial_embedding_table: nn.Parameter):\n",
    "        '''Make positional embedding table\n",
    "        \n",
    "        Args:\n",
    "            initial_embedding_table (Parameter): initialized embedding table.\n",
    "        \n",
    "        Returns:\n",
    "            pe (Parameter): position embedding table.\n",
    "        \n",
    "        '''\n",
    "        max_pos, emb_dim = initial_embedding_table.shape\n",
    "        pe = nn.Parameter(torch.zeros(max_pos, emb_dim))\n",
    "\n",
    "        pos_id = torch.arange(0, max_pos).unsqueeze(1)\n",
    "        freq = torch.pow(10000., -torch.arange(0, emb_dim, 2, dtype=torch.float) / emb_dim)\n",
    "        pos_freq = pos_id * freq\n",
    "        pe[:, 0::2] = torch.sin(pos_freq)\n",
    "        pe[:, 1::2] = torch.cos(pos_freq)\n",
    "        \n",
    "        pe.detach_()\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            input_ids (Tensor): shape '(batch_size, seq_len)'\n",
    "        \n",
    "        Return:\n",
    "            Tensor : shape '(seq_len, emb_dim)'\n",
    "        '''\n",
    "        batch_size, seq_len = input_ids.shape[:2]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n",
      "torch.Size([64, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pos = SinusoidalPositionalEncodedEmbedding(512, 512)\n",
    "# pos_encoding = pos(torch.zeros(2, 64, 512))\n",
    "# print(pos.weight.shape)\n",
    "# print(pos_encoding.shape)\n",
    "# plt.pcolormesh(pos_encoding.numpy(), cmap='RdBu')\n",
    "# plt.xlabel('Depth')\n",
    "# plt.xlim((0, 512))\n",
    "# plt.ylabel('Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        '''Initialize encoder layer\n",
    "        \n",
    "        Args:\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        # self multi-head attention\n",
    "        self.self_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                            num_heads = config.num_attention_heads,\n",
    "                                            drop_out = config.attention_drop_out)                      \n",
    "        self.attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        #position-wise feed forward\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(config.emb_dim,\n",
    "                                                               config.ffn_dim,\n",
    "                                                               config.drop_out)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "    \n",
    "    def forward(self, \n",
    "                x: torch.Tensor, \n",
    "                encoder_padding_mask: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, src_len, emb_dim)'\n",
    "            encoder_padding_mask (Tensor): binary BoolTensor. shape '(batch_size, src_len)'\n",
    "            \n",
    "        Returns:\n",
    "            x (Tensor): encoded output. shape '(batch_size, src_len, emb_dim)'\n",
    "            self_attn_weights: self attention socre\n",
    "        '''\n",
    "        residual = x\n",
    "        x, self_attn_weights = self.self_attn(query=x, \n",
    "                                              key=x, \n",
    "                                              attention_mask=encoder_padding_mask)\n",
    "        x = F.dropout(x, p=self.drop_out, training = self.training)\n",
    "        x = self.attn_layer_norm(x + residual)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.position_wise_feed_forward(x)\n",
    "        x = self.attn_layer_norm(x + residual)\n",
    "        \n",
    "#         clamping\n",
    "        if x.isnan().any() or x.isinf().any():\n",
    "            clamp_value = torch.finfo(x.dtype).max - 1000\n",
    "            x = torch.clamp(x, min = -clamp_value, max = clamp_value)\n",
    "        return x, self_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3, 12, 6)\n",
    "# padding_mask = torch.zeros(3,12, dtype=torch.bool)\n",
    "# padding_mask[0,6:] = True\n",
    "# padding_mask[1, 3:] = True\n",
    "# padding_mask[2, 10:] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 12, 12])\n",
      "tensor([[[[0.1708, 0.1567, 0.1699, 0.1591, 0.1672, 0.1764, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1740, 0.1586, 0.1638, 0.1631, 0.1702, 0.1703, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1769, 0.1530, 0.1657, 0.1593, 0.1714, 0.1738, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1707, 0.1583, 0.1694, 0.1611, 0.1684, 0.1722, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1772, 0.1543, 0.1615, 0.1599, 0.1699, 0.1772, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1666, 0.1606, 0.1741, 0.1611, 0.1666, 0.1710, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1748, 0.1532, 0.1679, 0.1582, 0.1696, 0.1763, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1694, 0.1567, 0.1746, 0.1595, 0.1686, 0.1711, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1778, 0.1511, 0.1666, 0.1581, 0.1719, 0.1746, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1799, 0.1546, 0.1581, 0.1620, 0.1721, 0.1733, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1794, 0.1552, 0.1580, 0.1623, 0.1718, 0.1733, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1799, 0.1574, 0.1537, 0.1642, 0.1712, 0.1736, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1453, 0.1895, 0.1787, 0.1771, 0.1525, 0.1570, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1416, 0.1953, 0.1803, 0.1719, 0.1596, 0.1513, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1559, 0.1768, 0.1773, 0.1637, 0.1737, 0.1525, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1414, 0.1945, 0.1820, 0.1736, 0.1575, 0.1510, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1498, 0.1834, 0.1799, 0.1725, 0.1599, 0.1545, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1417, 0.1974, 0.1726, 0.1767, 0.1508, 0.1607, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1438, 0.1903, 0.1826, 0.1766, 0.1537, 0.1530, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1490, 0.1861, 0.1773, 0.1690, 0.1645, 0.1541, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1535, 0.1816, 0.1718, 0.1709, 0.1608, 0.1614, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1496, 0.1838, 0.1810, 0.1668, 0.1688, 0.1499, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1402, 0.1959, 0.1826, 0.1750, 0.1554, 0.1509, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1421, 0.1934, 0.1819, 0.1744, 0.1565, 0.1517, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3192, 0.3671, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3234, 0.3629, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3195, 0.3584, 0.3221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3217, 0.3600, 0.3184, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3326, 0.3591, 0.3083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3272, 0.3683, 0.3045, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3123, 0.3754, 0.3123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3145, 0.3687, 0.3168, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3268, 0.3557, 0.3175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3275, 0.3585, 0.3141, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3261, 0.3692, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3255, 0.3727, 0.3018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3527, 0.3015, 0.3458, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3418, 0.3112, 0.3470, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3607, 0.2926, 0.3467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3861, 0.2827, 0.3311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3628, 0.2936, 0.3435, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3724, 0.2901, 0.3375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3747, 0.2982, 0.3271, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3805, 0.2923, 0.3272, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3517, 0.3013, 0.3470, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3648, 0.2932, 0.3419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3516, 0.3088, 0.3396, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3595, 0.3032, 0.3373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0954, 0.0976, 0.0985, 0.0968, 0.1078, 0.0920, 0.1039, 0.0996,\n",
      "           0.1037, 0.1048, 0.0000, 0.0000],\n",
      "          [0.0946, 0.0991, 0.1002, 0.0987, 0.1064, 0.0918, 0.1045, 0.0981,\n",
      "           0.1044, 0.1022, 0.0000, 0.0000],\n",
      "          [0.0959, 0.0961, 0.0969, 0.0970, 0.1074, 0.0896, 0.1049, 0.0994,\n",
      "           0.1055, 0.1073, 0.0000, 0.0000],\n",
      "          [0.0954, 0.0965, 0.0974, 0.0975, 0.1074, 0.0891, 0.1053, 0.0989,\n",
      "           0.1059, 0.1066, 0.0000, 0.0000],\n",
      "          [0.0955, 0.0990, 0.1000, 0.0966, 0.1074, 0.0953, 0.1024, 0.1000,\n",
      "           0.1015, 0.1024, 0.0000, 0.0000],\n",
      "          [0.0971, 0.0973, 0.0978, 0.0987, 0.1046, 0.0916, 0.1041, 0.0990,\n",
      "           0.1048, 0.1051, 0.0000, 0.0000],\n",
      "          [0.0946, 0.1009, 0.1021, 0.0984, 0.1061, 0.0959, 0.1026, 0.0986,\n",
      "           0.1016, 0.0991, 0.0000, 0.0000],\n",
      "          [0.0944, 0.1019, 0.1032, 0.0993, 0.1052, 0.0967, 0.1024, 0.0981,\n",
      "           0.1013, 0.0974, 0.0000, 0.0000],\n",
      "          [0.0922, 0.1042, 0.1060, 0.0997, 0.1062, 0.0977, 0.1024, 0.0971,\n",
      "           0.1006, 0.0939, 0.0000, 0.0000],\n",
      "          [0.0955, 0.0977, 0.0987, 0.0949, 0.1091, 0.0948, 0.1023, 0.1010,\n",
      "           0.1014, 0.1046, 0.0000, 0.0000],\n",
      "          [0.0920, 0.1030, 0.1049, 0.0988, 0.1075, 0.0960, 0.1031, 0.0974,\n",
      "           0.1014, 0.0959, 0.0000, 0.0000],\n",
      "          [0.0943, 0.1009, 0.1022, 0.0996, 0.1055, 0.0942, 0.1036, 0.0977,\n",
      "           0.1030, 0.0990, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1038, 0.1074, 0.1097, 0.1027, 0.0953, 0.1209, 0.0866, 0.1002,\n",
      "           0.0841, 0.0894, 0.0000, 0.0000],\n",
      "          [0.1036, 0.1050, 0.1114, 0.1079, 0.0955, 0.1148, 0.0902, 0.0944,\n",
      "           0.0839, 0.0932, 0.0000, 0.0000],\n",
      "          [0.1034, 0.1041, 0.1105, 0.1075, 0.0961, 0.1135, 0.0914, 0.0944,\n",
      "           0.0847, 0.0944, 0.0000, 0.0000],\n",
      "          [0.1018, 0.1028, 0.1025, 0.0984, 0.0988, 0.1109, 0.0937, 0.1028,\n",
      "           0.0929, 0.0954, 0.0000, 0.0000],\n",
      "          [0.1036, 0.1055, 0.1102, 0.1055, 0.0958, 0.1166, 0.0894, 0.0968,\n",
      "           0.0844, 0.0923, 0.0000, 0.0000],\n",
      "          [0.1046, 0.1058, 0.1149, 0.1102, 0.0942, 0.1201, 0.0874, 0.0924,\n",
      "           0.0788, 0.0915, 0.0000, 0.0000],\n",
      "          [0.1028, 0.1036, 0.1042, 0.0983, 0.0981, 0.1171, 0.0908, 0.1030,\n",
      "           0.0883, 0.0938, 0.0000, 0.0000],\n",
      "          [0.1038, 0.1068, 0.1105, 0.1045, 0.0953, 0.1193, 0.0874, 0.0983,\n",
      "           0.0837, 0.0904, 0.0000, 0.0000],\n",
      "          [0.1027, 0.1052, 0.1049, 0.0987, 0.0974, 0.1169, 0.0897, 0.1035,\n",
      "           0.0890, 0.0920, 0.0000, 0.0000],\n",
      "          [0.1032, 0.1072, 0.1059, 0.0981, 0.0964, 0.1208, 0.0869, 0.1049,\n",
      "           0.0874, 0.0893, 0.0000, 0.0000],\n",
      "          [0.1024, 0.1056, 0.1032, 0.0962, 0.0977, 0.1175, 0.0891, 0.1064,\n",
      "           0.0906, 0.0911, 0.0000, 0.0000],\n",
      "          [0.1024, 0.1012, 0.1054, 0.1031, 0.0985, 0.1101, 0.0951, 0.0973,\n",
      "           0.0891, 0.0979, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# temp_el = EncoderLayer(config)\n",
    "# out, attn_weights = temp_el(x, padding_mask)\n",
    "\n",
    "# '''패딩마스크가 잘 들어가고 있는지 확인.'''\n",
    "# print(attn_weights.shape)\n",
    "# print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 6])\n",
      "tensor([[[0.1341, 0.6254, 0.4068, 0.9600, 0.8427, 0.6127],\n",
      "         [0.9454, 0.3527, 0.5738, 0.4420, 0.0986, 0.1055],\n",
      "         [0.6148, 0.9632, 0.8235, 0.1746, 0.4425, 0.1030],\n",
      "         [0.8399, 0.4460, 0.6516, 0.6712, 0.3467, 0.4212],\n",
      "         [0.1260, 0.9768, 0.3339, 0.8040, 0.6599, 0.4534],\n",
      "         [0.4985, 0.0477, 0.7648, 0.6958, 0.6593, 0.0133],\n",
      "         [0.5415, 0.8647, 0.0937, 0.8432, 0.7894, 0.9058],\n",
      "         [0.7725, 0.2654, 0.6958, 0.0689, 0.3796, 0.3948],\n",
      "         [0.3498, 0.8196, 0.2033, 0.2753, 0.9089, 0.2407],\n",
      "         [0.6986, 0.9234, 0.5770, 0.3949, 0.1727, 0.0785],\n",
      "         [0.8277, 0.8205, 0.1837, 0.8197, 0.3422, 0.3128],\n",
      "         [0.4592, 0.6570, 0.1506, 0.7540, 0.1402, 0.1806]],\n",
      "\n",
      "        [[0.8069, 0.8808, 0.7280, 0.0267, 0.1839, 0.5527],\n",
      "         [0.1478, 0.3179, 0.9304, 0.3141, 0.5058, 0.0056],\n",
      "         [0.8253, 0.7874, 0.6853, 0.8367, 0.3622, 0.0280],\n",
      "         [0.7110, 0.2391, 0.0593, 0.7610, 0.2262, 0.8974],\n",
      "         [0.9768, 0.0563, 0.6656, 0.8145, 0.9520, 0.4474],\n",
      "         [0.8685, 0.4564, 0.9279, 0.8982, 0.7176, 0.7560],\n",
      "         [0.0981, 0.8050, 0.0887, 0.8581, 0.6271, 0.6767],\n",
      "         [0.1633, 0.5061, 0.0344, 0.6561, 0.2160, 0.8480],\n",
      "         [0.6251, 0.1315, 0.4774, 0.4724, 0.6226, 0.1188],\n",
      "         [0.4951, 0.0616, 0.9702, 0.5442, 0.1680, 0.4941],\n",
      "         [0.1402, 0.1245, 0.9164, 0.1454, 0.4987, 0.6312],\n",
      "         [0.6537, 0.3945, 0.5768, 0.1261, 0.6806, 0.9906]],\n",
      "\n",
      "        [[0.7644, 0.8146, 0.7146, 0.7728, 0.2708, 0.5741],\n",
      "         [0.6609, 0.7699, 0.6739, 0.1421, 0.3845, 0.5530],\n",
      "         [0.4224, 0.9854, 0.5874, 0.3149, 0.3785, 0.1601],\n",
      "         [0.2799, 0.8404, 0.0738, 0.3954, 0.7571, 0.2832],\n",
      "         [0.2584, 0.1346, 0.9224, 0.2624, 0.0167, 0.4649],\n",
      "         [0.9676, 0.8544, 0.6755, 0.2740, 0.0387, 0.3082],\n",
      "         [0.5507, 0.3037, 0.9663, 0.7596, 0.6355, 0.2489],\n",
      "         [0.7614, 0.0379, 0.7198, 0.0622, 0.0809, 0.9981],\n",
      "         [0.6838, 0.0789, 0.9440, 0.4434, 0.6615, 0.9260],\n",
      "         [0.3282, 0.2825, 0.8621, 0.9591, 0.1489, 0.4976],\n",
      "         [0.7197, 0.3925, 0.9118, 0.8074, 0.8490, 0.8726],\n",
      "         [0.3043, 0.3313, 0.7419, 0.0822, 0.7066, 0.1970]]])\n"
     ]
    }
   ],
   "source": [
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 embedding_table: nn.Embedding):\n",
    "        '''Initialize stack of Encoder layers\n",
    "        \n",
    "        Args:\n",
    "            config (Config):Configuration parameters.\n",
    "            embedding_table (nn.Embedding): instance of nn.Embedding for Encoder input tokens.\n",
    "                                            input tokens shape '(batch_size, src_len)'\n",
    "                                            embedding table shape '(num_voca, emb_dim)'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        self.embedding_table = embedding_table\n",
    "        self.embed_positions = SinusoidalPositionalEncodedEmbedding(config.max_position,\n",
    "                                                                    config.emb_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_encoder_layers)])                     \n",
    "        \n",
    "    def forward(self, \n",
    "                input_indices: torch.Tensor, \n",
    "                padding_mask = None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_indices (Tensor): input to Encoder. shape '(batch_size, src_len)'\n",
    "            padding_mask (Tensor): padding mask. shape '(batch_size, src_len)'\n",
    "            \n",
    "        Returns:\n",
    "            x (Tensor): Encoder output. shape '(batch_size, src_len, emb_dim)'\n",
    "            self_attn_scores (list): list of attention weights of each Encoder layer.\n",
    "        '''\n",
    "        \n",
    "        inputs_embed = self.embedding_table(input_indices)\n",
    "        pos_embed = self.embed_positions(input_indices)\n",
    "        x = inputs_embed + pos_embed\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        \n",
    "        self_attn_weights = []\n",
    "        for encoder_layer in self.layers:\n",
    "            x, attn_weights = encoder_layer(x, padding_mask)\n",
    "            self_attn_weights.append(attn_weights.detach().clone())\n",
    "        return x, self_attn_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 4, 1, 5],\n",
      "        [6, 8, 1, 2],\n",
      "        [2, 6, 5, 6]])\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.randint(0,9,(3,4))\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,2:] = True\n",
    "# padding_mask[2,3:] = True\n",
    "# print(x)\n",
    "# print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 6])\n",
      "tensor([[[-1.3025, -0.9928,  0.2050,  1.5568, -0.3290,  0.8625],\n",
      "         [ 1.9995,  0.1273, -0.2641,  0.0126, -0.6571, -1.2182],\n",
      "         [ 0.0792, -1.7066,  0.0700,  1.2234, -0.7058,  1.0398],\n",
      "         [ 0.2989, -2.1989,  0.4415,  0.7441,  0.1678,  0.5466]],\n",
      "\n",
      "        [[ 0.5390, -0.2083,  1.1824, -1.3140, -1.2224,  1.0233],\n",
      "         [ 0.6253, -1.5964,  0.1947,  1.6283, -0.3615, -0.4903],\n",
      "         [-0.0444, -1.9817, -0.0388,  1.1166,  0.0425,  0.9058],\n",
      "         [ 1.2310, -0.7670, -0.2509,  0.7056, -1.6663,  0.7476]],\n",
      "\n",
      "        [[ 0.4257, -1.0316,  0.3692,  1.7124, -1.2842, -0.1915],\n",
      "         [ 0.2288, -1.3521,  1.2362,  1.0247,  0.1000, -1.2375],\n",
      "         [ 0.4074, -2.1207,  0.1661,  0.9412, -0.0429,  0.6490],\n",
      "         [-0.1673, -2.0074,  0.8460,  1.0839,  0.2267,  0.0180]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "# emb = nn.Embedding(10, 6)\n",
    "# e = Encoder(config, emb)\n",
    "# enc_out, attn_scores = e(x, padding_mask)\n",
    "# print(enc_out.shape)\n",
    "# print(enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4, 4])\n",
      "[tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6238, 0.3762, 0.0000, 0.0000],\n",
      "          [0.3326, 0.6674, 0.0000, 0.0000],\n",
      "          [0.5759, 0.4241, 0.0000, 0.0000],\n",
      "          [0.4770, 0.5230, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3876, 0.6124, 0.0000, 0.0000],\n",
      "          [0.3440, 0.6560, 0.0000, 0.0000],\n",
      "          [0.2348, 0.7652, 0.0000, 0.0000],\n",
      "          [0.5448, 0.4552, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3027, 0.4947, 0.2026, 0.0000],\n",
      "          [0.2206, 0.4599, 0.3196, 0.0000],\n",
      "          [0.2692, 0.6210, 0.1098, 0.0000],\n",
      "          [0.2185, 0.6726, 0.1089, 0.0000]],\n",
      "\n",
      "         [[0.4806, 0.1744, 0.3451, 0.0000],\n",
      "          [0.3268, 0.2277, 0.4455, 0.0000],\n",
      "          [0.1105, 0.6443, 0.2452, 0.0000],\n",
      "          [0.1587, 0.5101, 0.3312, 0.0000]]]]), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5192, 0.4808, 0.0000, 0.0000],\n",
      "          [0.5608, 0.4392, 0.0000, 0.0000],\n",
      "          [0.5073, 0.4927, 0.0000, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4940, 0.5060, 0.0000, 0.0000],\n",
      "          [0.4506, 0.5494, 0.0000, 0.0000],\n",
      "          [0.5466, 0.4534, 0.0000, 0.0000],\n",
      "          [0.4772, 0.5228, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3353, 0.2869, 0.3778, 0.0000],\n",
      "          [0.3550, 0.2238, 0.4212, 0.0000],\n",
      "          [0.3044, 0.2801, 0.4155, 0.0000],\n",
      "          [0.3323, 0.2344, 0.4333, 0.0000]],\n",
      "\n",
      "         [[0.4160, 0.3701, 0.2139, 0.0000],\n",
      "          [0.4334, 0.2897, 0.2769, 0.0000],\n",
      "          [0.4695, 0.2690, 0.2615, 0.0000],\n",
      "          [0.4957, 0.2274, 0.2769, 0.0000]]]]), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3345, 0.6655, 0.0000, 0.0000],\n",
      "          [0.6760, 0.3240, 0.0000, 0.0000],\n",
      "          [0.5964, 0.4036, 0.0000, 0.0000],\n",
      "          [0.4907, 0.5093, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4628, 0.5372, 0.0000, 0.0000],\n",
      "          [0.7047, 0.2953, 0.0000, 0.0000],\n",
      "          [0.6558, 0.3442, 0.0000, 0.0000],\n",
      "          [0.6078, 0.3922, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3626, 0.1902, 0.4472, 0.0000],\n",
      "          [0.2939, 0.2504, 0.4557, 0.0000],\n",
      "          [0.4048, 0.1850, 0.4102, 0.0000],\n",
      "          [0.3385, 0.1936, 0.4678, 0.0000]],\n",
      "\n",
      "         [[0.3341, 0.3183, 0.3476, 0.0000],\n",
      "          [0.3449, 0.3071, 0.3479, 0.0000],\n",
      "          [0.3019, 0.3696, 0.3285, 0.0000],\n",
      "          [0.3115, 0.3536, 0.3349, 0.0000]]]])]\n"
     ]
    }
   ],
   "source": [
    "# print(attn_scores[0].shape)\n",
    "# print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        '''Initialize decoder layer\n",
    "        \n",
    "        Args:\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        # masked multi_head attention\n",
    "        self.self_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                            num_heads = config.num_attention_heads,\n",
    "                                            drop_out = config.attention_drop_out,\n",
    "                                            causal = True)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                                       num_heads = config.num_attention_heads,\n",
    "                                                       drop_out = config.attention_drop_out,\n",
    "                                                       encoder_decoder_attention = True)\n",
    "        self.enc_dec_attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        #position-wise feed forward\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(config.emb_dim,\n",
    "                                                               config.ffn_dim,\n",
    "                                                               config.drop_out)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                encoder_output: torch.Tensor,\n",
    "                enc_dec_attention_padding_mask: torch.Tensor = None,\n",
    "                causal_mask: torch.Tensor = None):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): Input to decoder layer. shape '(batch_size, trg_len, emb_dim)'.\n",
    "            encoder_output (Tensor): Output of encoder. shape '(batch_size, src_len, emb_dim)'\n",
    "            enc_dec_attention_padding_mask (Tensor): Binary BoolTensor for masking padding of\n",
    "                                                     encoder output.\n",
    "                                                     shape '(batch_size, src_len)'.\n",
    "            causal_mask (Tensor): Binary BoolTensor for masking future information in decoder.\n",
    "                                  shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            x (Tensor): Output of decoder layer. shape '(batch_size, trg_len, emb_dim)'.\n",
    "            self_attn_weights (Tensor): Masked self attention weights of decoder. \n",
    "                                        shape '(batch_size, trg_len, trg_len)'.\n",
    "            enc_dec_attn_weights (Tensor): Encoder-decoder attention weights.\n",
    "                                           shape '(batch_size, trg_len, src_len)'.\n",
    "        '''\n",
    "        \n",
    "        # msked self attention\n",
    "        residual = x\n",
    "        x, self_attn_weights = self.self_attn(query = x,\n",
    "                                              key = x,\n",
    "                                              attention_mask = causal_mask)\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        x = self.self_attn_layer_norm(x + residual)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        residual = x\n",
    "        x, enc_dec_attn_weights = self.enc_dec_attn(query = x,\n",
    "                                                    key = encoder_output,\n",
    "                                                    attention_mask = enc_dec_attention_padding_mask)\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        x = self.enc_dec_attn_layer_norm(x + residual)\n",
    "        \n",
    "        # position-wise feed forward\n",
    "        residual = x\n",
    "        x = self.position_wise_feed_forward(x)\n",
    "        x = self.feed_forward_layer_norm(x + residual)\n",
    "        \n",
    "        return x, self_attn_weights, enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False,  True,  True,  True],\n",
       "        [False, False,  True,  True]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.rand(3, 5, 6) \n",
    "# '''batch = 3, trg_len = 5, emb_dim = 6'''\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,1:] = True\n",
    "# padding_mask[2,2:] = True\n",
    "# padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# causal_mask = torch.zeros(5,5, dtype=torch.bool) # shape (trg_len, trg_len)\n",
    "# causal_mask[0,1:] = True\n",
    "# causal_mask[1,2:] = True\n",
    "# causal_mask[2,3:] = True\n",
    "# causal_mask[3,4:] = True\n",
    "\n",
    "# causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = DecoderLayer(config)\n",
    "# dec_out, self_attn_weigths, enc_dec_attn_weights = d(x,enc_out,padding_mask,causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4602, -0.2589, -0.7205,  0.2710,  1.7388,  0.4297],\n",
       "         [ 1.7172, -0.3285, -1.1042,  0.9358, -0.3824, -0.8379],\n",
       "         [-0.6947,  0.0496,  1.6494,  0.9311, -1.1329, -0.8026],\n",
       "         [ 0.2296, -0.7104, -0.0132,  0.3849,  1.6806, -1.5715],\n",
       "         [-0.5435, -1.6973,  0.6637,  1.3384,  0.6502, -0.4114]],\n",
       "\n",
       "        [[-0.9488,  0.7812,  0.9702,  1.1254, -1.4240, -0.5039],\n",
       "         [ 1.8887, -1.0337, -0.5487,  0.7111, -0.6496, -0.3678],\n",
       "         [ 0.0378, -2.1498,  0.2023,  0.3559,  0.8088,  0.7450],\n",
       "         [-1.0966, -0.0915,  0.4868,  1.8291, -1.0980, -0.0298],\n",
       "         [-0.1955, -1.0700, -0.3840,  1.9630, -0.7759,  0.4624]],\n",
       "\n",
       "        [[-0.9225,  0.0157, -0.7864,  2.1081, -0.1956, -0.2193],\n",
       "         [ 0.0030,  1.3493,  0.3979, -0.3792, -1.8974,  0.5264],\n",
       "         [-0.6530, -1.4063,  0.0632,  0.5760, -0.3511,  1.7712],\n",
       "         [-0.5537, -0.6644, -0.0535,  1.1963, -1.3436,  1.4188],\n",
       "         [-0.9675, -0.0936,  1.0875,  1.3001, -1.4702,  0.1438]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dec_out.shape)\n",
    "# dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5059, 0.4941, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3445, 0.3303, 0.3252, 0.0000, 0.0000],\n",
       "          [0.2707, 0.2602, 0.2461, 0.2230, 0.0000],\n",
       "          [0.2222, 0.2095, 0.1982, 0.1718, 0.1983]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4872, 0.5128, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3447, 0.3570, 0.2982, 0.0000, 0.0000],\n",
       "          [0.2523, 0.2823, 0.2227, 0.2428, 0.0000],\n",
       "          [0.2156, 0.2271, 0.1840, 0.1932, 0.1801]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5234, 0.4766, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3651, 0.3182, 0.3166, 0.0000, 0.0000],\n",
       "          [0.2813, 0.2324, 0.2150, 0.2713, 0.0000],\n",
       "          [0.2148, 0.1826, 0.1731, 0.2110, 0.2184]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4715, 0.5285, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3180, 0.3669, 0.3151, 0.0000, 0.0000],\n",
       "          [0.2550, 0.2790, 0.2432, 0.2227, 0.0000],\n",
       "          [0.2017, 0.2251, 0.1980, 0.1768, 0.1984]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4984, 0.5016, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3488, 0.3541, 0.2971, 0.0000, 0.0000],\n",
       "          [0.2511, 0.2516, 0.2150, 0.2824, 0.0000],\n",
       "          [0.2031, 0.2044, 0.1596, 0.2310, 0.2020]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4914, 0.5086, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3347, 0.3505, 0.3148, 0.0000, 0.0000],\n",
       "          [0.2597, 0.2632, 0.2388, 0.2383, 0.0000],\n",
       "          [0.2126, 0.2158, 0.1945, 0.1935, 0.1836]]]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(self_attn_weigths.shape)\n",
    "# self_attn_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.3207, 0.6793, 0.0000, 0.0000],\n",
       "          [0.4695, 0.5305, 0.0000, 0.0000],\n",
       "          [0.4135, 0.5865, 0.0000, 0.0000],\n",
       "          [0.3864, 0.6136, 0.0000, 0.0000],\n",
       "          [0.3992, 0.6008, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.5453, 0.4547, 0.0000, 0.0000],\n",
       "          [0.7371, 0.2629, 0.0000, 0.0000],\n",
       "          [0.5486, 0.4514, 0.0000, 0.0000],\n",
       "          [0.6009, 0.3991, 0.0000, 0.0000],\n",
       "          [0.7042, 0.2958, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(enc_dec_attn_weights.shape) \n",
    "# '''(batch_size, num_head, trg_len, src_len)'''\n",
    "# enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 embedding_table: nn.Embedding):\n",
    "        '''Initialize stack of Encoder layers\n",
    "        \n",
    "        Args:\n",
    "            config (Config):Configuration parameters.\n",
    "            embedding_table (nn.Embedding): instance of nn.Embedding for Decoder input tokens.\n",
    "                                            input tokens shape '(batch_size, trg_len)'\n",
    "                                            embedding table shape '(num_voca, emb_dim)'\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        self.embedding_table = embedding_table\n",
    "        self.embed_positions = SinusoidalPositionalEncodedEmbedding(config.max_position,\n",
    "                                                                    config.emb_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_decoder_layers)])\n",
    "    \n",
    "    def forward(self,\n",
    "                input_indices: torch.Tensor,\n",
    "                encoder_output: torch.Tensor,\n",
    "                enc_dec_attention_padding_mask: torch.Tensor = None,\n",
    "                causal_mask: torch.Tensor = None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_indeces (Tensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "            encoder_output (Tensor): output of encoder. shape '(batch_size, src_len, emb_dim)'\n",
    "            enc_dec_attention_padding_masl (Tensor): Binary BoolTensor for masking padding of\n",
    "                                                     encoder output.\n",
    "                                                     shape '(batch_size, src_len)'.\n",
    "            causal_mask (Tensor): Binary BoolTensor for masking future information in decoder.\n",
    "                                  shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            x (Tensor): output of decoder. shape '(batch_size, trg_len, emb_dim)'\n",
    "            enc_dec_attn_weigths (list): list of enc-dec attention weights of each Decoder layer.\n",
    "        '''\n",
    "        \n",
    "        inputs_embed = self.embedding_table(input_indices)\n",
    "        pos_embed = self.embed_positions(input_indices)\n",
    "        x = inputs_embed + pos_embed\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        \n",
    "        enc_dec_attn_weights = []\n",
    "        for decoder_layer in self.layers:\n",
    "            x, _, attn_weights = decoder_layer(x, \n",
    "                                               encoder_output,\n",
    "                                               enc_dec_attention_padding_mask,\n",
    "                                               causal_mask)\n",
    "            enc_dec_attn_weights.append(attn_weights.detach().clone())\n",
    "        return x, enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False,  True,  True,  True],\n",
       "        [False, False, False,  True]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.randint(0,9,(3,5)) # batch_size = 3  trg_len = 4\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,1:] = True\n",
    "# padding_mask[2,3:] = True\n",
    "# padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# causal_mask = torch.zeros(5,5, dtype=torch.bool) # shape (trg_len, trg_len)\n",
    "# causal_mask[0,1:] = True\n",
    "# causal_mask[1,2:] = True\n",
    "# causal_mask[2,3:] = True\n",
    "# causal_mask[3,4:] = True\n",
    "\n",
    "# causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = nn.Embedding(10, 6)\n",
    "# d = Decoder(config, emb)\n",
    "# dec_out, enc_dec_attn_weights = d(x, enc_out, padding_mask, causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0478, -0.6920, -1.2126, -0.5023,  1.8531,  0.6016],\n",
       "         [ 0.4506, -0.3274,  0.2383,  1.6283, -1.7026, -0.2873],\n",
       "         [ 0.6402, -0.7487, -0.9670,  1.8017,  0.1776, -0.9037],\n",
       "         [-0.3533, -1.6914, -0.6217,  0.8091,  1.2809,  0.5765],\n",
       "         [-1.0378, -0.5244, -0.9240,  1.8080,  0.7234, -0.0452]],\n",
       "\n",
       "        [[ 1.3166,  0.1410, -1.5426, -0.8937, -0.0536,  1.0323],\n",
       "         [-0.7094, -0.0701, -0.4296,  0.8628, -1.3273,  1.6736],\n",
       "         [-1.6697, -0.6589,  0.8599,  0.9348, -0.4479,  0.9818],\n",
       "         [-1.1978, -1.2878,  0.9483, -0.3545,  0.7312,  1.1607],\n",
       "         [-1.6284, -0.9305,  0.6264, -0.0839,  1.1669,  0.8495]],\n",
       "\n",
       "        [[-0.1937,  0.3954, -1.2673,  1.7563,  0.3167, -1.0075],\n",
       "         [-1.0743,  0.1025, -0.2158,  2.0340, -0.0400, -0.8064],\n",
       "         [-0.2710, -1.3996, -0.8336,  1.6176,  0.8059,  0.0807],\n",
       "         [-1.1914, -0.9092,  0.2883,  1.7591,  0.5627, -0.5095],\n",
       "         [-0.9126, -0.0539, -0.5029,  1.9745,  0.4126, -0.9178]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dec_out.shape)\n",
    "# dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.1381, 0.5199, 0.3420, 0.0000],\n",
       "           [0.1592, 0.4762, 0.3646, 0.0000],\n",
       "           [0.2736, 0.3518, 0.3746, 0.0000],\n",
       "           [0.2285, 0.3858, 0.3857, 0.0000],\n",
       "           [0.1748, 0.4379, 0.3873, 0.0000]],\n",
       " \n",
       "          [[0.3544, 0.3377, 0.3079, 0.0000],\n",
       "           [0.3574, 0.3400, 0.3026, 0.0000],\n",
       "           [0.3204, 0.3456, 0.3340, 0.0000],\n",
       "           [0.3519, 0.3500, 0.2981, 0.0000],\n",
       "           [0.3540, 0.3596, 0.2864, 0.0000]]]]),\n",
       " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.2819, 0.2853, 0.4328, 0.0000],\n",
       "           [0.3395, 0.3277, 0.3328, 0.0000],\n",
       "           [0.2294, 0.3256, 0.4451, 0.0000],\n",
       "           [0.3366, 0.3262, 0.3372, 0.0000],\n",
       "           [0.2900, 0.3223, 0.3877, 0.0000]],\n",
       " \n",
       "          [[0.2996, 0.3666, 0.3338, 0.0000],\n",
       "           [0.3186, 0.3616, 0.3197, 0.0000],\n",
       "           [0.3206, 0.3771, 0.3023, 0.0000],\n",
       "           [0.3312, 0.3596, 0.3092, 0.0000],\n",
       "           [0.3117, 0.3726, 0.3157, 0.0000]]]]),\n",
       " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3419, 0.2636, 0.3945, 0.0000],\n",
       "           [0.3477, 0.2713, 0.3811, 0.0000],\n",
       "           [0.3489, 0.2957, 0.3553, 0.0000],\n",
       "           [0.3549, 0.2973, 0.3478, 0.0000],\n",
       "           [0.3495, 0.2707, 0.3797, 0.0000]],\n",
       " \n",
       "          [[0.3161, 0.3487, 0.3352, 0.0000],\n",
       "           [0.2823, 0.4198, 0.2979, 0.0000],\n",
       "           [0.3449, 0.2780, 0.3771, 0.0000],\n",
       "           [0.2817, 0.4281, 0.2901, 0.0000],\n",
       "           [0.2883, 0.3999, 0.3118, 0.0000]]]])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(enc_dec_attn_weights[0].shape)\n",
    "# enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 SRC: Field, \n",
    "                 TRG: Field, \n",
    "                 config):\n",
    "        '''Initialize transformer\n",
    "        \n",
    "        Args:\n",
    "            SRC (Field): source data class\n",
    "            TRG (Field): target data class\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.SRC = SRC\n",
    "        self.TRG = TRG\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(len(SRC.vocab), \n",
    "                                          config.emb_dim,\n",
    "                                          padding_idx = SRC.vocab.stoi['<pad>'])\n",
    "        self.dec_embedding = nn.Embedding(len(TRG.vocab), \n",
    "                                          config.emb_dim,\n",
    "                                          padding_idx = TRG.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.encoder = Encoder(config, self.enc_embedding)\n",
    "        self.decoder = Decoder(config, self.dec_embedding)\n",
    "        \n",
    "        self.linear = nn.Linear(config.emb_dim, len(TRG.vocab))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'weigth' in name:\n",
    "                    nn.init.normal_(param.data, mean = 0, std = 0.01)\n",
    "                else:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    def generate_mask(self, \n",
    "                      src: torch.LongTensor, \n",
    "                      trg: torch.LongTensor):\n",
    "        '''Generate padding mask and causal mask\n",
    "        \n",
    "        Args:\n",
    "            src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "            trg (LongTensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            padding_mask (Tensor): shape '(batch_size, src_len)'\n",
    "            causal_mask (Tensor): shape '(trg_len, trg_len)'\n",
    "        '''\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # padding mask\n",
    "        padding_mask = src.eq(self.SRC.vocab.stoi['<pad']).to(device)\n",
    "        # causal mask\n",
    "        tmp = torch.ones(trg.size(1), trg.size(1), dtype = torch.bool)\n",
    "#         mask = torch.arange(tmp.size(-1))\n",
    "#         causal_mask = tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), False).to(device)\n",
    "        causal_mask = torch.tril(tmp,-1).transpose(0,1).contiguous().to(device)\n",
    "        return padding_mask, causal_mask\n",
    "    \n",
    "    def forward(self,\n",
    "                src: torch.LongTensor,\n",
    "                trg: torch.LongTensor):\n",
    "        '''\n",
    "        Args:\n",
    "            src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "            trg (LongTensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            output (Tensor): output of transformer. \n",
    "                             shape '(batch_size, trg_len, # trg vocab)'\n",
    "            encoder_attn_weights (list): list of attention weights of each Encoder layer.\n",
    "            enc_dec_attn_weights (list): list of enc-dec attention weights of each Decoder layer.\n",
    "        '''\n",
    "        \n",
    "        padding_mask, causal_mask = self.generate_mask(src, trg)\n",
    "        \n",
    "        encoder_output, encoder_attn_weights = self.encoder(input_indices = src,\n",
    "                                                            padding_mask = padding_mask)\n",
    "        \n",
    "        decoder_output, enc_dec_attn_weights = self.decoder(input_indices = trg,\n",
    "                                                            encoder_output = encoder_output,\n",
    "                                                            enc_dec_attention_padding_mask = padding_mask,\n",
    "                                                            causal_mask = causal_mask)\n",
    "        \n",
    "        output = self.linear(decoder_output)\n",
    "        \n",
    "        return output, encoder_attn_weights, enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \"emb_dim\":64,\n",
    "    \"ffn_dim\":256,\n",
    "    \"num_attention_heads\":4,\n",
    "    \"attention_drop_out\":0.0,\n",
    "    \"drop_out\":0.2,\n",
    "    \"max_position\":512,\n",
    "    \"num_encoder_layers\":3,\n",
    "    \"num_decoder_layers\":3,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':5e-4,\n",
    "    'n_epochs':100,\n",
    "    'gradient_clip':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "def prepare_data(batch_size):\n",
    "    '''prepare data\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): batch size.\n",
    "        \n",
    "    Returns:\n",
    "        SRC (Field): source data Field class\n",
    "        TRG (Field): target data Field class\n",
    "        train_iterator (BucketIterator): training data iterator\n",
    "        valid_iterator (BucketIterator): validation data iterator\n",
    "        test_iterator (BucketIterator): test data iterator\n",
    "    '''\n",
    "    \n",
    "    SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            batch_first=True,\n",
    "            lower = True)\n",
    "\n",
    "    TRG = Field(tokenize = \"spacy\",\n",
    "                tokenizer_language=\"en\",\n",
    "                init_token = '<sos>',\n",
    "                eos_token = '<eos>',\n",
    "                batch_first=True,\n",
    "                lower = True)\n",
    "\n",
    "    train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                        fields = (SRC, TRG))\n",
    "\n",
    "    SRC.build_vocab(train_data, min_freq = 2)\n",
    "    TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size = batch_size,\n",
    "        device = device,\n",
    "        shuffle=True)\n",
    "    \n",
    "    data_loders = dict()\n",
    "    data_loders['train'] = train_iterator\n",
    "    data_loders['val'] = valid_iterator\n",
    "    data_loders['test'] = test_iterator\n",
    "    \n",
    "    return SRC, TRG, data_loders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(SRC: Field,\n",
    "                TRG: Field,\n",
    "                config):\n",
    "    '''Get network.\n",
    "    \n",
    "    Args:\n",
    "        SRC (Field): source data Field class.\n",
    "        TRG (Field): target data Field class.\n",
    "        config (Config): configuration parameters.\n",
    "    \n",
    "    Returns:\n",
    "        model (Module): transformer model.\n",
    "        criterion (CrossEntropyLoss): loss function. \n",
    "        optimizer (Adam): optimizer.\n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Transformer(SRC, TRG, config).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG.vocab.stoi['<pad>'])\n",
    "    optimizer = optim.Adam(model.parameters(),lr = config.learning_rate)\n",
    "    \n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          data_loders: dict,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          config):\n",
    "    '''Training model\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): transformer model.\n",
    "        data_loders (dict): training/validation data iterator.\n",
    "        criterion : loss function. \n",
    "        optimizer : optimizer.\n",
    "        config (Config): configuration parameters.\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print_loss_every = 1\n",
    "    for epoch in range(config.n_epochs): \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            loss_val_sum = 0\n",
    "            \n",
    "            for batch in data_loders[phase]:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    " \n",
    "                src = batch.src.to(device)\n",
    "                trg = batch.trg.to(device)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output, _, _ = model(src, trg)\n",
    "\n",
    "                    output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "                    trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "                    loss = criterion(output, trg)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                    # gradient clipping\n",
    "#                         torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                        optimizer.step()\n",
    "\n",
    "                loss_val_sum += loss\n",
    "\n",
    "            if ((epoch % print_loss_every) == 0) or (epoch == (config.n_epochs - 1)):\n",
    "                loss_val_avg = loss_val_sum / len(data_loders[phase])\n",
    "                print(\n",
    "                    f\"epoch:[{epoch+1}/{config.n_epochs}] {phase} cost:[{loss_val_avg:.3f}]\"\n",
    "                )\n",
    "    print('training done!!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TRG, data_loders = prepare_data(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = get_network(SRC, TRG, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1/100] train cost:[8.604]\n",
      "epoch:[1/100] val cost:[8.524]\n",
      "epoch:[2/100] train cost:[8.451]\n",
      "epoch:[2/100] val cost:[8.371]\n",
      "epoch:[3/100] train cost:[8.303]\n",
      "epoch:[3/100] val cost:[8.222]\n",
      "epoch:[4/100] train cost:[8.158]\n",
      "epoch:[4/100] val cost:[8.077]\n",
      "epoch:[5/100] train cost:[8.016]\n",
      "epoch:[5/100] val cost:[7.936]\n",
      "epoch:[6/100] train cost:[7.879]\n",
      "epoch:[6/100] val cost:[7.798]\n",
      "epoch:[7/100] train cost:[7.746]\n",
      "epoch:[7/100] val cost:[7.665]\n",
      "epoch:[8/100] train cost:[7.617]\n",
      "epoch:[8/100] val cost:[7.536]\n",
      "epoch:[9/100] train cost:[7.492]\n",
      "epoch:[9/100] val cost:[7.411]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-19d37b9a3ed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-59cc2ee669ef>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data_loders, criterion, optimizer, config)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                     \u001b[1;31m# gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m#                         torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(model, data_loders, criterion, optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:09<15:25,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.604 | Train PPL: 5453.463\n",
      "\t Val. Loss: 8.524 |  Val. PPL: 5035.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                                | 2/100 [00:18<15:16,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.451 | Train PPL: 4681.116\n",
      "\t Val. Loss: 8.371 |  Val. PPL: 4321.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                               | 3/100 [00:27<15:05,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.302 | Train PPL: 4033.552\n",
      "\t Val. Loss: 8.222 |  Val. PPL: 3722.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                              | 4/100 [00:37<14:54,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.157 | Train PPL: 3488.838\n",
      "\t Val. Loss: 8.077 |  Val. PPL: 3218.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                              | 4/100 [00:44<17:53, 11.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f97c9758a1b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-f97c9758a1b8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, enc_attention_scores, _ = model(src, trg)\n",
    "\n",
    "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, attention_score, _ = model(src, trg) #turn off teacher forcing\n",
    "\n",
    "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in tqdm(range(config.n_epochs), total=config.n_epochs):\n",
    "    train_loss = train(model, data_loders['train'], optimizer, criterion, config.gradient_clip)\n",
    "    valid_loss = evaluate(model, data_loders['val'], criterion)\n",
    "    \n",
    "#     if best_valid_loss < valid_loss:\n",
    "#         break\n",
    "#     else:\n",
    "#         best_valid_loss = valid_loss\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, data_loders['test'], criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_embedding.weight \t torch.Size([7854, 64])\n",
      "dec_embedding.weight \t torch.Size([5893, 64])\n",
      "encoder.embedding_table.weight \t torch.Size([7854, 64])\n",
      "encoder.embed_positions.weight \t torch.Size([512, 64])\n",
      "encoder.layers.0.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.0.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.0.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.0.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.1.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.1.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.1.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.1.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.2.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.2.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.2.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.2.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.embedding_table.weight \t torch.Size([5893, 64])\n",
      "decoder.embed_positions.weight \t torch.Size([512, 64])\n",
      "decoder.layers.0.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.0.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.0.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.1.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.2.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "linear.weight \t torch.Size([5893, 64])\n",
      "linear.bias \t torch.Size([5893])\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param_tensor in model.state_dict():\n",
    "    i+= 1\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             data_loders: dict,\n",
    "             criterion,\n",
    "             optimizer,\n",
    "             config):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
