{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import easydict\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_dim: int, \n",
    "                 num_heads: int, \n",
    "                 drop_out: float = 0.0,\n",
    "                 bias: bool = False, \n",
    "                 encoder_decoder_attention: bool = False,\n",
    "                 causal: bool = False):\n",
    "        '''Initialize MultiHeadAttention class variables.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim (int): Dimension of a word * number of heads.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            drop_out (float): Drop out rate.\n",
    "            bias (bool): Boolean that indicating whether to use bias or not.\n",
    "            encoder_decoder_attention (bool): Boolean that indicating whether the multi head\n",
    "                                              attention is encoder-decoder attention or not.\n",
    "            causal (bool): Boolean that indicating whether to use causal mask or not.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        assert self.head_dim * num_heads == emb_dim, \"emb_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.drop_out = drop_out\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        self.causal = causal\n",
    "        \n",
    "        self.wk = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.wq = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.wv = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "        self.output = nn.Linear(self.emb_dim, self.emb_dim, bias = bias)\n",
    "    \n",
    "    def multi_head_scaled_dot_product(self,\n",
    "                                      query: torch.Tensor,\n",
    "                                      key: torch.Tensor,\n",
    "                                      value: torch.Tensor,\n",
    "                                      attention_mask: torch.BoolTensor):\n",
    "        '''Perform multi-head version of scaled dot product.\n",
    "        \n",
    "        Args:\n",
    "            query (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            key (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            value (Tensor): shape '(batch size, # attention head, seqence length, demension of head)'\n",
    "            attention_mask: This mask can be either causal mask or padding mask.\n",
    "                            shape '(batch size, source squence length)' for padding mask.\n",
    "                            shape '(sequence length, target sequence length)' for causal mask.\n",
    "        Returns:\n",
    "            attn_output (Tensor): output of attention mechanism. shape '(batch size, seq_len, emb_dim)'\n",
    "            attn_weights (Tensor): value of attention weight of each word. shape '(batch size, # attn head, seq_len, seq_len)'\n",
    "        '''\n",
    "        \n",
    "        attn_weights = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "        '''shape of attn_weights : (batch size, # attn head, seq_len, seq_len)'''\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            if self.causal:\n",
    "                '''Masking future info for encoder-decoder attention.'''\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n",
    "                '''\n",
    "                shape of attention_mask : (trg_len, trg_len).\n",
    "                shape of attention_mask.unsqueeze(0).unsqueeze(1) : (1, 1, trg_len, trg_len).\n",
    "                '''\n",
    "            else:\n",
    "                '''Masking padding token so that it is not used for attention.'''\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "                '''\n",
    "                shape of attention_mask : (batch_size, src_len)\n",
    "                shape of attention_mask.unsqueeze(1).unsqueeze(2) : (batch_size, 1, 1, src_len)\n",
    "                '''\n",
    "        attn_weights = F.softmax(attn_weights, dim = -1)\n",
    "        attn_probs = F.dropout(attn_weights, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_probs, value)\n",
    "        '''shape of attn_output : (batch size, # attn head, seq_len, head_dim)'''\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
    "        '''shape of attn_output : (batch size, seq_len, # attn head, head_dim)'''\n",
    "        shape = attn_output.size()[:-2] + (self.emb_dim,)\n",
    "        attn_output = attn_output.view(*shape)\n",
    "        '''shape of attn_output : (batch size, seq_len, emb_dim)'''\n",
    "        attn_output = self.output(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def transform_to_multi_head(self, \n",
    "                                x: torch.Tensor):\n",
    "        ''' Reshape input\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: shape '(batch_size, # attn head, seq_len, head_dim)'\n",
    "        '''\n",
    "        \n",
    "        shape = x.size()[:-1] + (self.num_heads, self.head_dim,)\n",
    "        x = x.view(*shape)\n",
    "        \n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                attention_mask: torch.Tensor = None):\n",
    "        '''\n",
    "        Args:\n",
    "            query (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "            key (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "            attention_mask (Tensor): shape '(batch size, squence length)' for padding mask.\n",
    "                                     shape '(sequence length, sequence length)' for causal mask.\n",
    "        \n",
    "        Returns:\n",
    "            attn_output (Tensor): output of attention mechanism. shape '(batch size, seq_len, emb_dim)'\n",
    "            attn_weights (Tensor): value of attention weight of each word. shape '(batch size, # attn head, seq_l\n",
    "        '''\n",
    "        \n",
    "        q = self.wq(query)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        if self.encoder_decoder_attention:\n",
    "            '''\n",
    "            query is output of encoder\n",
    "            key is input of decoder\n",
    "            '''\n",
    "            k = self.wk(key)\n",
    "            v = self.wv(key)\n",
    "        \n",
    "        # self attention\n",
    "        else:\n",
    "            '''\n",
    "            Both of query and key are input of encoder(query is same with key).\n",
    "            '''\n",
    "            k = self.wk(query)\n",
    "            v = self.wv(query)\n",
    "        \n",
    "        q = self.transform_to_multi_head(q)\n",
    "        k = self.transform_to_multi_head(k)\n",
    "        v = self.transform_to_multi_head(v)\n",
    "        \n",
    "        attn_output, attn_weights = self.multi_head_scaled_dot_product(q,k,v,attention_mask)\n",
    "            \n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_mha = MultiHeadAttention(emb_dim=512, num_heads=8)\n",
    "# x = torch.rand(3, 45, 512)\n",
    "# out, attn = temp_mha(query=x, key=x, attention_mask=None)\n",
    "# print(out.size(), attn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb_dim: int,\n",
    "                 hid_dim: int,\n",
    "                 drop_out: float = 0.1):\n",
    "        '''Initialize position-wise feed forward network.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim (int): word embdding dimension.\n",
    "            hid_dim (int): hidden dimesion.\n",
    "            drop_out (float): drop out rate.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(emb_dim, hid_dim)\n",
    "        self.linear_2 = nn.Linear(hid_dim, emb_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "    \n",
    "    def forward(self,\n",
    "                 x: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        \n",
    "        Return:\n",
    "            x (Tensor): shape '(batch_size, seq_len, emb_dim)'\n",
    "        '''\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        x = self.linear_2(x)\n",
    "        x = F.dropout(x, p=self.drop_out, training=self.training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_pwff = PositionWiseFeedForward(512, 256)\n",
    "# x = torch.rand(3, 45, 512)\n",
    "# out = temp_pwff(x)\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncodedEmbedding(nn.Embedding):\n",
    "    def __init__(self, \n",
    "                 max_position: int, \n",
    "                 embedding_dim: int):\n",
    "        '''Initialize positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            max_position (int): maximum length of input sequence length.\n",
    "                                 That is, it can encode position up to max_positions'th position.\n",
    "            embedding_dim (int): embedding dimension.\n",
    "        '''\n",
    "        super().__init__(max_position, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    def _init_weight(self, initial_embedding_table: nn.Parameter):\n",
    "        '''Make positional embedding table\n",
    "        \n",
    "        Args:\n",
    "            initial_embedding_table (Parameter): initialized embedding table.\n",
    "        \n",
    "        Returns:\n",
    "            pe (Parameter): position embedding table.\n",
    "        \n",
    "        '''\n",
    "        max_pos, emb_dim = initial_embedding_table.shape\n",
    "        pe = nn.Parameter(torch.zeros(max_pos, emb_dim))\n",
    "\n",
    "        pos_id = torch.arange(0, max_pos).unsqueeze(1)\n",
    "        freq = torch.pow(10000., -torch.arange(0, emb_dim, 2, dtype=torch.float) / emb_dim)\n",
    "        pos_freq = pos_id * freq\n",
    "        pe[:, 0::2] = torch.sin(pos_freq)\n",
    "        pe[:, 1::2] = torch.cos(pos_freq)\n",
    "        \n",
    "        pe.detach_()\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            input_ids (Tensor): shape '(batch_size, seq_len)'\n",
    "        \n",
    "        Return:\n",
    "            Tensor : shape '(seq_len, emb_dim)'\n",
    "        '''\n",
    "        batch_size, seq_len = input_ids.shape[:2]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = SinusoidalPositionalEncodedEmbedding(512, 512)\n",
    "# pos_encoding = pos(torch.zeros(2, 64, 512))\n",
    "# print(pos.weight.shape)\n",
    "# print(pos_encoding.shape)\n",
    "# plt.pcolormesh(pos_encoding.numpy(), cmap='RdBu')\n",
    "# plt.xlabel('Depth')\n",
    "# plt.xlim((0, 512))\n",
    "# plt.ylabel('Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        '''Initialize encoder layer\n",
    "        \n",
    "        Args:\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        # self multi-head attention\n",
    "        self.self_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                            num_heads = config.num_attention_heads,\n",
    "                                            drop_out = config.attention_drop_out)                      \n",
    "        self.attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        #position-wise feed forward\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(config.emb_dim,\n",
    "                                                               config.ffn_dim,\n",
    "                                                               config.drop_out)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "    \n",
    "    def forward(self, \n",
    "                x: torch.Tensor, \n",
    "                encoder_padding_mask: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): shape '(batch_size, src_len, emb_dim)'\n",
    "            encoder_padding_mask (Tensor): binary BoolTensor. shape '(batch_size, src_len)'\n",
    "            \n",
    "        Returns:\n",
    "            x (Tensor): encoded output. shape '(batch_size, src_len, emb_dim)'\n",
    "            self_attn_weights: self attention socre\n",
    "        '''\n",
    "        residual = x\n",
    "        x, self_attn_weights = self.self_attn(query=x, \n",
    "                                              key=x, \n",
    "                                              attention_mask=encoder_padding_mask)\n",
    "        x = F.dropout(x, p=self.drop_out, training = self.training)\n",
    "        x = self.attn_layer_norm(x + residual)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.position_wise_feed_forward(x)\n",
    "        x = self.attn_layer_norm(x + residual)\n",
    "        \n",
    "#         clamping\n",
    "        if x.isnan().any() or x.isinf().any():\n",
    "            clamp_value = torch.finfo(x.dtype).max - 1000\n",
    "            x = torch.clamp(x, min = -clamp_value, max = clamp_value)\n",
    "        return x, self_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3, 12, 6)\n",
    "# padding_mask = torch.zeros(3,12, dtype=torch.bool)\n",
    "# padding_mask[0,6:] = True\n",
    "# padding_mask[1, 3:] = True\n",
    "# padding_mask[2, 10:] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_el = EncoderLayer(config)\n",
    "# out, attn_weights = temp_el(x, padding_mask)\n",
    "\n",
    "# '''패딩마스크가 잘 들어가고 있는지 확인.'''\n",
    "# print(attn_weights.shape)\n",
    "# print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 embedding_table: nn.Embedding):\n",
    "        '''Initialize stack of Encoder layers\n",
    "        \n",
    "        Args:\n",
    "            config (Config):Configuration parameters.\n",
    "            embedding_table (nn.Embedding): instance of nn.Embedding for Encoder input tokens.\n",
    "                                            input tokens shape '(batch_size, src_len)'\n",
    "                                            embedding table shape '(num_voca, emb_dim)'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        self.embedding_table = embedding_table\n",
    "        self.embed_positions = SinusoidalPositionalEncodedEmbedding(config.max_position,\n",
    "                                                                    config.emb_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_encoder_layers)])                     \n",
    "        \n",
    "    def forward(self, \n",
    "                input_indices: torch.Tensor, \n",
    "                padding_mask = None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_indices (Tensor): input to Encoder. shape '(batch_size, src_len)'\n",
    "            padding_mask (Tensor): padding mask. shape '(batch_size, src_len)'\n",
    "            \n",
    "        Returns:\n",
    "            x (Tensor): Encoder output. shape '(batch_size, src_len, emb_dim)'\n",
    "            self_attn_scores (list): list of attention weights of each Encoder layer.\n",
    "        '''\n",
    "        \n",
    "        inputs_embed = self.embedding_table(input_indices)\n",
    "        pos_embed = self.embed_positions(input_indices)\n",
    "        x = inputs_embed + pos_embed\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        \n",
    "        self_attn_weights = []\n",
    "        for encoder_layer in self.layers:\n",
    "            x, attn_weights = encoder_layer(x, padding_mask)\n",
    "            self_attn_weights.append(attn_weights.detach().clone())\n",
    "        return x, self_attn_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(0,9,(3,4))\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,2:] = True\n",
    "# padding_mask[2,3:] = True\n",
    "# print(x)\n",
    "# print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = nn.Embedding(10, 6)\n",
    "# e = Encoder(config, emb)\n",
    "# enc_out, attn_scores = e(x, padding_mask)\n",
    "# print(enc_out.shape)\n",
    "# print(enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attn_scores[0].shape)\n",
    "# print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        '''Initialize decoder layer\n",
    "        \n",
    "        Args:\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        # masked multi_head attention\n",
    "        self.self_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                            num_heads = config.num_attention_heads,\n",
    "                                            drop_out = config.attention_drop_out,\n",
    "                                            causal = True)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(emb_dim = config.emb_dim,\n",
    "                                                       num_heads = config.num_attention_heads,\n",
    "                                                       drop_out = config.attention_drop_out,\n",
    "                                                       encoder_decoder_attention = True)\n",
    "        self.enc_dec_attn_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        #position-wise feed forward\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(config.emb_dim,\n",
    "                                                               config.ffn_dim,\n",
    "                                                               config.drop_out)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                encoder_output: torch.Tensor,\n",
    "                enc_dec_attention_padding_mask: torch.Tensor = None,\n",
    "                causal_mask: torch.Tensor = None):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "            x (Tensor): Input to decoder layer. shape '(batch_size, trg_len, emb_dim)'.\n",
    "            encoder_output (Tensor): Output of encoder. shape '(batch_size, src_len, emb_dim)'\n",
    "            enc_dec_attention_padding_mask (Tensor): Binary BoolTensor for masking padding of\n",
    "                                                     encoder output.\n",
    "                                                     shape '(batch_size, src_len)'.\n",
    "            causal_mask (Tensor): Binary BoolTensor for masking future information in decoder.\n",
    "                                  shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            x (Tensor): Output of decoder layer. shape '(batch_size, trg_len, emb_dim)'.\n",
    "            self_attn_weights (Tensor): Masked self attention weights of decoder. \n",
    "                                        shape '(batch_size, trg_len, trg_len)'.\n",
    "            enc_dec_attn_weights (Tensor): Encoder-decoder attention weights.\n",
    "                                           shape '(batch_size, trg_len, src_len)'.\n",
    "        '''\n",
    "        \n",
    "        # msked self attention\n",
    "        residual = x\n",
    "        x, self_attn_weights = self.self_attn(query = x,\n",
    "                                              key = x,\n",
    "                                              attention_mask = causal_mask)\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        x = self.self_attn_layer_norm(x + residual)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        residual = x\n",
    "        x, enc_dec_attn_weights = self.enc_dec_attn(query = x,\n",
    "                                                    key = encoder_output,\n",
    "                                                    attention_mask = enc_dec_attention_padding_mask)\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        x = self.enc_dec_attn_layer_norm(x + residual)\n",
    "        \n",
    "        # position-wise feed forward\n",
    "        residual = x\n",
    "        x = self.position_wise_feed_forward(x)\n",
    "        x = self.feed_forward_layer_norm(x + residual)\n",
    "        \n",
    "        return x, self_attn_weights, enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3, 5, 6) \n",
    "# '''batch = 3, trg_len = 5, emb_dim = 6'''\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,1:] = True\n",
    "# padding_mask[2,2:] = True\n",
    "# padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal_mask = torch.zeros(5,5, dtype=torch.bool) # shape (trg_len, trg_len)\n",
    "# causal_mask[0,1:] = True\n",
    "# causal_mask[1,2:] = True\n",
    "# causal_mask[2,3:] = True\n",
    "# causal_mask[3,4:] = True\n",
    "\n",
    "# causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = DecoderLayer(config)\n",
    "# dec_out, self_attn_weigths, enc_dec_attn_weights = d(x,enc_out,padding_mask,causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dec_out.shape)\n",
    "# dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(self_attn_weigths.shape)\n",
    "# self_attn_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(enc_dec_attn_weights.shape) \n",
    "# '''(batch_size, num_head, trg_len, src_len)'''\n",
    "# enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 embedding_table: nn.Embedding):\n",
    "        '''Initialize stack of Encoder layers\n",
    "        \n",
    "        Args:\n",
    "            config (Config):Configuration parameters.\n",
    "            embedding_table (nn.Embedding): instance of nn.Embedding for Decoder input tokens.\n",
    "                                            input tokens shape '(batch_size, trg_len)'\n",
    "                                            embedding table shape '(num_voca, emb_dim)'\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_out = config.drop_out\n",
    "        \n",
    "        self.embedding_table = embedding_table\n",
    "        self.embed_positions = SinusoidalPositionalEncodedEmbedding(config.max_position,\n",
    "                                                                    config.emb_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_decoder_layers)])\n",
    "    \n",
    "    def forward(self,\n",
    "                input_indices: torch.Tensor,\n",
    "                encoder_output: torch.Tensor,\n",
    "                enc_dec_attention_padding_mask: torch.Tensor = None,\n",
    "                causal_mask: torch.Tensor = None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_indeces (Tensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "            encoder_output (Tensor): output of encoder. shape '(batch_size, src_len, emb_dim)'\n",
    "            enc_dec_attention_padding_masl (Tensor): Binary BoolTensor for masking padding of\n",
    "                                                     encoder output.\n",
    "                                                     shape '(batch_size, src_len)'.\n",
    "            causal_mask (Tensor): Binary BoolTensor for masking future information in decoder.\n",
    "                                  shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            x (Tensor): output of decoder. shape '(batch_size, trg_len, emb_dim)'\n",
    "            enc_dec_attn_weigths (list): list of enc-dec attention weights of each Decoder layer.\n",
    "        '''\n",
    "        \n",
    "        inputs_embed = self.embedding_table(input_indices)\n",
    "        pos_embed = self.embed_positions(input_indices)\n",
    "        x = inputs_embed + pos_embed\n",
    "        x = F.dropout(x, p = self.drop_out, training = self.training)\n",
    "        \n",
    "        enc_dec_attn_weights = []\n",
    "        for decoder_layer in self.layers:\n",
    "            x, _, attn_weights = decoder_layer(x, \n",
    "                                               encoder_output,\n",
    "                                               enc_dec_attention_padding_mask,\n",
    "                                               causal_mask)\n",
    "            enc_dec_attn_weights.append(attn_weights.detach().clone())\n",
    "        return x, enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = easydict.EasyDict({\n",
    "#     \"emb_dim\":6,\n",
    "#     \"ffn_dim\":256,\n",
    "#     \"num_attention_heads\":2,\n",
    "#     \"attention_drop_out\":0.0,\n",
    "#     \"drop_out\":0.2,\n",
    "#     \"max_position\":512,\n",
    "#     \"num_encoder_layers\":3,\n",
    "#     \"num_decoder_layers\":3,\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(0,9,(3,5)) # batch_size = 3  trg_len = 4\n",
    "# padding_mask = torch.zeros(3,4, dtype=torch.bool)\n",
    "# padding_mask[0,1:] = True\n",
    "# padding_mask[1,1:] = True\n",
    "# padding_mask[2,3:] = True\n",
    "# padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal_mask = torch.zeros(5,5, dtype=torch.bool) # shape (trg_len, trg_len)\n",
    "# causal_mask[0,1:] = True\n",
    "# causal_mask[1,2:] = True\n",
    "# causal_mask[2,3:] = True\n",
    "# causal_mask[3,4:] = True\n",
    "\n",
    "# causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = nn.Embedding(10, 6)\n",
    "# d = Decoder(config, emb)\n",
    "# dec_out, enc_dec_attn_weights = d(x, enc_out, padding_mask, causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dec_out.shape)\n",
    "# dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(enc_dec_attn_weights[0].shape)\n",
    "# enc_dec_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 SRC: Field, \n",
    "                 TRG: Field, \n",
    "                 config):\n",
    "        '''Initialize transformer\n",
    "        \n",
    "        Args:\n",
    "            SRC (Field): source data class\n",
    "            TRG (Field): target data class\n",
    "            config (Config): configuration parameters.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.SRC = SRC\n",
    "        self.TRG = TRG\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(len(SRC.vocab), \n",
    "                                          config.emb_dim,\n",
    "                                          padding_idx = SRC.vocab.stoi['<pad>'])\n",
    "        self.dec_embedding = nn.Embedding(len(TRG.vocab), \n",
    "                                          config.emb_dim,\n",
    "                                          padding_idx = TRG.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.encoder = Encoder(config, self.enc_embedding)\n",
    "        self.decoder = Decoder(config, self.dec_embedding)\n",
    "        \n",
    "        self.linear = nn.Linear(config.emb_dim, len(TRG.vocab))\n",
    "        \n",
    "#         self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'weigth' in name:\n",
    "                    nn.init.normal_(param.data, mean = 0, std = 0.01)\n",
    "                else:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "    \n",
    "#     def generate_mask(self, \n",
    "#                       src: torch.LongTensor, \n",
    "#                       trg: torch.LongTensor):\n",
    "#         '''Generate padding mask and causal mask\n",
    "        \n",
    "#         Args:\n",
    "#             src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "#             trg (LongTensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "        \n",
    "#         Returns:\n",
    "#             padding_mask (Tensor): shape '(batch_size, src_len)'\n",
    "#             causal_mask (Tensor): shape '(trg_len, trg_len)'\n",
    "#         '''\n",
    "        \n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#         # padding mask\n",
    "#         padding_mask = src.eq(self.SRC.vocab.stoi['<pad>']).to(device)\n",
    "#         # causal mask\n",
    "#         tmp = torch.ones(trg.size(1), trg.size(1), dtype = torch.bool)\n",
    "# #         mask = torch.arange(tmp.size(-1))\n",
    "# #         causal_mask = tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), False).to(device)\n",
    "#         causal_mask = torch.tril(tmp,-1).transpose(0,1).contiguous().to(device)\n",
    "#         return padding_mask, causal_mask\n",
    "    \n",
    "    def generate_causal_mask(self,  \n",
    "                             trg: torch.LongTensor):\n",
    "        '''Generate padding mask and causal mask\n",
    "        \n",
    "        Args:\n",
    "            trg (LongTensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            causal_mask (Tensor): shape '(trg_len, trg_len)'\n",
    "        '''\n",
    "        tmp = torch.ones(trg.size(1), trg.size(1), dtype = torch.bool)\n",
    "        causal_mask = torch.tril(tmp,-1).transpose(0,1).contiguous().to(self.device)\n",
    "        \n",
    "        return causal_mask\n",
    "    \n",
    "    def generate_padding_mask(self, \n",
    "                              src: torch.LongTensor):\n",
    "        '''Generate padding mask\n",
    "        \n",
    "        Args:\n",
    "            src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "        \n",
    "        Returns:\n",
    "            padding_mask (Tensor): shape '(batch_size, src_len)'\n",
    "        '''\n",
    "        padding_mask = src.eq(self.SRC.vocab.stoi['<pad>']).to(self.device)\n",
    "        \n",
    "        return padding_mask\n",
    "    \n",
    "    def forward(self,\n",
    "                src: torch.LongTensor,\n",
    "                trg: torch.LongTensor):\n",
    "        '''\n",
    "        Args:\n",
    "            src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "            trg (LongTensor): input to decoder. shape '(batch_size, trg_len)'\n",
    "        \n",
    "        Returns:\n",
    "            output (Tensor): output of transformer. \n",
    "                             shape '(batch_size, trg_len, # trg vocab)'\n",
    "            encoder_attn_weights (list): list of attention weights of each Encoder layer.\n",
    "            enc_dec_attn_weights (list): list of enc-dec attention weights of each Decoder layer.\n",
    "        '''\n",
    "        \n",
    "#         padding_mask, causal_mask = self.generate_mask(src, trg)\n",
    "        \n",
    "        padding_mask = self.generate_padding_mask(src)\n",
    "        causal_mask = self.generate_causal_mask(trg)\n",
    "        \n",
    "        encoder_output, encoder_attn_weights = self.encoder(input_indices = src,\n",
    "                                                            padding_mask = padding_mask)\n",
    "        \n",
    "        decoder_output, enc_dec_attn_weights = self.decoder(input_indices = trg,\n",
    "                                                            encoder_output = encoder_output,\n",
    "                                                            enc_dec_attention_padding_mask = padding_mask,\n",
    "                                                            causal_mask = causal_mask)\n",
    "        \n",
    "        output = self.linear(decoder_output)\n",
    "        \n",
    "        return output, encoder_attn_weights, enc_dec_attn_weights\n",
    "    \n",
    "    def predict(self,\n",
    "                src: torch.LongTensor):\n",
    "        '''\n",
    "        Args:\n",
    "            src (LongTensor): input to encoder. shape '(batch_size, src_len)'\n",
    "        \n",
    "        Returns:\n",
    "            output_tokens (LongTensor): predicted tokens. shape'(batch_size, max_position)'\n",
    "        '''\n",
    "        padding_mask = self.generate_padding_mask(src)\n",
    "        \n",
    "        encoder_output, _ = self.encoder(input_indices = src,\n",
    "                                         padding_mask = padding_mask)\n",
    "        output_tokens = (torch.ones((self.config.batch_size, self.config.max_position))\\\n",
    "                         * self.TRG.vocab.stoi['<pad>']).long().to(self.device) \n",
    "        ## (batch_size, max_position)\n",
    "        output_tokens[:,0] = self.TRG.vocab.stoi['<sos>']\n",
    "        for trg_index in range(1, self.config.max_position):\n",
    "            trg = output_tokens[:,:trg_index] # (batch_size, trg_index)\n",
    "            causal_mask = self.generate_causal_mask(trg) # (trg_index, trg_index)\n",
    "            output, _ = self.decoder(input_indices = trg,\n",
    "                                             encoder_output = encoder_output,\n",
    "                                             enc_dec_attention_padding_mask = padding_mask,\n",
    "                                             causal_mask = causal_mask) # (batch_size, trg_index, emb_dim)\n",
    "            output = self.linear(output) # (batch_size, trg_index, # trg vocab)\n",
    "            output = torch.argmax(output, dim = -1) # (batch_size, trg_index)\n",
    "            output_tokens[:,trg_index] = output[:,-1]\n",
    "        \n",
    "        return output_tokens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \"emb_dim\":32,\n",
    "    \"ffn_dim\":128,\n",
    "    \"num_attention_heads\":2,\n",
    "    \"attention_drop_out\":0.0,\n",
    "    \"drop_out\":0.2,\n",
    "    \"max_position\":256,\n",
    "    \"num_encoder_layers\":3,\n",
    "    \"num_decoder_layers\":3,\n",
    "    'batch_size':64,\n",
    "    'learning_rate':5e-4,\n",
    "    'n_epochs':200,\n",
    "    'gradient_clip':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "def prepare_data(batch_size):\n",
    "    '''prepare data\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): batch size.\n",
    "        \n",
    "    Returns:\n",
    "        SRC (Field): source data Field class\n",
    "        TRG (Field): target data Field class\n",
    "        train_iterator (BucketIterator): training data iterator\n",
    "        valid_iterator (BucketIterator): validation data iterator\n",
    "        test_iterator (BucketIterator): test data iterator\n",
    "    '''\n",
    "    \n",
    "    SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            batch_first=True,\n",
    "            lower = True)\n",
    "\n",
    "    TRG = Field(tokenize = \"spacy\",\n",
    "                tokenizer_language=\"en\",\n",
    "                init_token = '<sos>',\n",
    "                eos_token = '<eos>',\n",
    "                batch_first=True,\n",
    "                lower = True)\n",
    "\n",
    "    train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                        fields = (SRC, TRG))\n",
    "\n",
    "    SRC.build_vocab(train_data, min_freq = 2)\n",
    "    TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size = batch_size,\n",
    "        device = device,\n",
    "        shuffle=True)\n",
    "    \n",
    "    data_loders = dict()\n",
    "    data_loders['train'] = train_iterator\n",
    "    data_loders['val'] = valid_iterator\n",
    "    data_loders['test'] = test_iterator\n",
    "    \n",
    "    return SRC, TRG, data_loders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(SRC: Field,\n",
    "                TRG: Field,\n",
    "                config):\n",
    "    '''Get network.\n",
    "    \n",
    "    Args:\n",
    "        SRC (Field): source data Field class.\n",
    "        TRG (Field): target data Field class.\n",
    "        config (Config): configuration parameters.\n",
    "    \n",
    "    Returns:\n",
    "        model (Module): transformer model.\n",
    "        criterion (CrossEntropyLoss): loss function. \n",
    "        optimizer (Adam): optimizer.\n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Transformer(SRC, TRG, config).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG.vocab.stoi['<pad>'])\n",
    "    optimizer = optim.Adam(model.parameters(),lr = config.learning_rate)\n",
    "    \n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          data_loaders: dict,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          config):\n",
    "    '''Training model\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): transformer model.\n",
    "        data_loaders (dict): training/validation data iterator.\n",
    "        criterion : loss function. \n",
    "        optimizer : optimizer.\n",
    "        config (Config): configuration parameters.\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print_loss_every = 1\n",
    "    for epoch in range(config.n_epochs): \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            loss_val_sum = 0\n",
    "            \n",
    "            for batch in data_loaders[phase]:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    " \n",
    "                src = batch.src.to(device)\n",
    "                trg = batch.trg.to(device)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output, _, _ = model(src, trg)\n",
    "\n",
    "                    output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "                    trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "                    loss = criterion(output, trg)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                    # gradient clipping\n",
    "#                         torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                        optimizer.step()\n",
    "\n",
    "                loss_val_sum += loss\n",
    "\n",
    "            if ((epoch % print_loss_every) == 0) or (epoch == (config.n_epochs - 1)):\n",
    "                loss_val_avg = loss_val_sum / len(data_loders[phase])\n",
    "                print(\n",
    "                    f\"epoch:[{epoch+1}/{config.n_epochs}] {phase} cost:[{loss_val_avg:.3f}]\"\n",
    "                )\n",
    "    print('training done!!')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "def save_model(model: nn.Module,\n",
    "               optimizer,\n",
    "               epoch: int):\n",
    "    if not os.path.isdir('ckpt'):\n",
    "        os.mkdir('ckpt')\n",
    "    state = {'epoch' : epoch, \n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict()}\n",
    "    now = time.localtime()\n",
    "    path = 'ckpt/' + f\"{now.tm_year}-{now.tm_mon}-{now.tm_mday}_{now.tm_hour}_{now.tm_min}_{now.tm_sec}\" + '.pt'\n",
    "\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TRG, data_loaders = prepare_data(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = get_network(SRC, TRG, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1/200] train cost:[11.415]\n",
      "epoch:[1/200] val cost:[9.094]\n",
      "epoch:[2/200] train cost:[8.869]\n",
      "epoch:[2/200] val cost:[7.890]\n",
      "epoch:[3/200] train cost:[8.138]\n",
      "epoch:[3/200] val cost:[7.329]\n",
      "epoch:[4/200] train cost:[7.714]\n",
      "epoch:[4/200] val cost:[6.919]\n",
      "epoch:[5/200] train cost:[7.420]\n",
      "epoch:[5/200] val cost:[6.668]\n",
      "epoch:[6/200] train cost:[7.197]\n",
      "epoch:[6/200] val cost:[6.454]\n",
      "epoch:[7/200] train cost:[7.002]\n",
      "epoch:[7/200] val cost:[6.274]\n",
      "epoch:[8/200] train cost:[6.840]\n",
      "epoch:[8/200] val cost:[6.103]\n",
      "epoch:[9/200] train cost:[6.689]\n",
      "epoch:[9/200] val cost:[5.968]\n",
      "epoch:[10/200] train cost:[6.558]\n",
      "epoch:[10/200] val cost:[5.871]\n",
      "epoch:[11/200] train cost:[6.435]\n",
      "epoch:[11/200] val cost:[5.736]\n",
      "epoch:[12/200] train cost:[6.324]\n",
      "epoch:[12/200] val cost:[5.630]\n",
      "epoch:[13/200] train cost:[6.227]\n",
      "epoch:[13/200] val cost:[5.531]\n",
      "epoch:[14/200] train cost:[6.133]\n",
      "epoch:[14/200] val cost:[5.456]\n",
      "epoch:[15/200] train cost:[6.042]\n",
      "epoch:[15/200] val cost:[5.394]\n",
      "epoch:[16/200] train cost:[5.962]\n",
      "epoch:[16/200] val cost:[5.300]\n",
      "epoch:[17/200] train cost:[5.887]\n",
      "epoch:[17/200] val cost:[5.244]\n",
      "epoch:[18/200] train cost:[5.818]\n",
      "epoch:[18/200] val cost:[5.195]\n",
      "epoch:[19/200] train cost:[5.750]\n",
      "epoch:[19/200] val cost:[5.127]\n",
      "epoch:[20/200] train cost:[5.680]\n",
      "epoch:[20/200] val cost:[5.068]\n",
      "epoch:[21/200] train cost:[5.622]\n",
      "epoch:[21/200] val cost:[5.014]\n",
      "epoch:[22/200] train cost:[5.566]\n",
      "epoch:[22/200] val cost:[4.984]\n",
      "epoch:[23/200] train cost:[5.514]\n",
      "epoch:[23/200] val cost:[4.930]\n",
      "epoch:[24/200] train cost:[5.464]\n",
      "epoch:[24/200] val cost:[4.901]\n",
      "epoch:[25/200] train cost:[5.409]\n",
      "epoch:[25/200] val cost:[4.851]\n",
      "epoch:[26/200] train cost:[5.362]\n",
      "epoch:[26/200] val cost:[4.806]\n",
      "epoch:[27/200] train cost:[5.316]\n",
      "epoch:[27/200] val cost:[4.769]\n",
      "epoch:[28/200] train cost:[5.271]\n",
      "epoch:[28/200] val cost:[4.735]\n",
      "epoch:[29/200] train cost:[5.230]\n",
      "epoch:[29/200] val cost:[4.701]\n",
      "epoch:[30/200] train cost:[5.193]\n",
      "epoch:[30/200] val cost:[4.658]\n",
      "epoch:[31/200] train cost:[5.147]\n",
      "epoch:[31/200] val cost:[4.644]\n",
      "epoch:[32/200] train cost:[5.106]\n",
      "epoch:[32/200] val cost:[4.596]\n",
      "epoch:[33/200] train cost:[5.072]\n",
      "epoch:[33/200] val cost:[4.561]\n",
      "epoch:[34/200] train cost:[5.043]\n",
      "epoch:[34/200] val cost:[4.549]\n",
      "epoch:[35/200] train cost:[5.004]\n",
      "epoch:[35/200] val cost:[4.501]\n",
      "epoch:[36/200] train cost:[4.970]\n",
      "epoch:[36/200] val cost:[4.501]\n",
      "epoch:[37/200] train cost:[4.942]\n",
      "epoch:[37/200] val cost:[4.463]\n",
      "epoch:[38/200] train cost:[4.905]\n",
      "epoch:[38/200] val cost:[4.429]\n",
      "epoch:[39/200] train cost:[4.874]\n",
      "epoch:[39/200] val cost:[4.420]\n",
      "epoch:[40/200] train cost:[4.846]\n",
      "epoch:[40/200] val cost:[4.395]\n",
      "epoch:[41/200] train cost:[4.818]\n",
      "epoch:[41/200] val cost:[4.375]\n",
      "epoch:[42/200] train cost:[4.790]\n",
      "epoch:[42/200] val cost:[4.344]\n",
      "epoch:[43/200] train cost:[4.766]\n",
      "epoch:[43/200] val cost:[4.335]\n",
      "epoch:[44/200] train cost:[4.742]\n",
      "epoch:[44/200] val cost:[4.321]\n",
      "epoch:[45/200] train cost:[4.711]\n",
      "epoch:[45/200] val cost:[4.285]\n",
      "epoch:[46/200] train cost:[4.689]\n",
      "epoch:[46/200] val cost:[4.274]\n",
      "epoch:[47/200] train cost:[4.663]\n",
      "epoch:[47/200] val cost:[4.289]\n",
      "epoch:[48/200] train cost:[4.639]\n",
      "epoch:[48/200] val cost:[4.242]\n",
      "epoch:[49/200] train cost:[4.622]\n",
      "epoch:[49/200] val cost:[4.227]\n",
      "epoch:[50/200] train cost:[4.597]\n",
      "epoch:[50/200] val cost:[4.214]\n",
      "epoch:[51/200] train cost:[4.571]\n",
      "epoch:[51/200] val cost:[4.219]\n",
      "epoch:[52/200] train cost:[4.552]\n",
      "epoch:[52/200] val cost:[4.196]\n",
      "epoch:[53/200] train cost:[4.537]\n",
      "epoch:[53/200] val cost:[4.162]\n",
      "epoch:[54/200] train cost:[4.512]\n",
      "epoch:[54/200] val cost:[4.148]\n",
      "epoch:[55/200] train cost:[4.486]\n",
      "epoch:[55/200] val cost:[4.148]\n",
      "epoch:[56/200] train cost:[4.468]\n",
      "epoch:[56/200] val cost:[4.131]\n",
      "epoch:[57/200] train cost:[4.451]\n",
      "epoch:[57/200] val cost:[4.113]\n",
      "epoch:[58/200] train cost:[4.434]\n",
      "epoch:[58/200] val cost:[4.125]\n",
      "epoch:[59/200] train cost:[4.415]\n",
      "epoch:[59/200] val cost:[4.097]\n",
      "epoch:[60/200] train cost:[4.399]\n",
      "epoch:[60/200] val cost:[4.079]\n",
      "epoch:[61/200] train cost:[4.376]\n",
      "epoch:[61/200] val cost:[4.080]\n",
      "epoch:[62/200] train cost:[4.354]\n",
      "epoch:[62/200] val cost:[4.063]\n",
      "epoch:[63/200] train cost:[4.340]\n",
      "epoch:[63/200] val cost:[4.042]\n",
      "epoch:[64/200] train cost:[4.321]\n",
      "epoch:[64/200] val cost:[4.027]\n",
      "epoch:[65/200] train cost:[4.305]\n",
      "epoch:[65/200] val cost:[4.033]\n",
      "epoch:[66/200] train cost:[4.288]\n",
      "epoch:[66/200] val cost:[4.027]\n",
      "epoch:[67/200] train cost:[4.271]\n",
      "epoch:[67/200] val cost:[4.009]\n",
      "epoch:[68/200] train cost:[4.252]\n",
      "epoch:[68/200] val cost:[4.004]\n",
      "epoch:[69/200] train cost:[4.240]\n",
      "epoch:[69/200] val cost:[3.987]\n",
      "epoch:[70/200] train cost:[4.228]\n",
      "epoch:[70/200] val cost:[3.997]\n",
      "epoch:[71/200] train cost:[4.212]\n",
      "epoch:[71/200] val cost:[3.963]\n",
      "epoch:[72/200] train cost:[4.195]\n",
      "epoch:[72/200] val cost:[3.949]\n",
      "epoch:[73/200] train cost:[4.180]\n",
      "epoch:[73/200] val cost:[3.955]\n",
      "epoch:[74/200] train cost:[4.169]\n",
      "epoch:[74/200] val cost:[3.948]\n",
      "epoch:[75/200] train cost:[4.157]\n",
      "epoch:[75/200] val cost:[3.918]\n",
      "epoch:[76/200] train cost:[4.144]\n",
      "epoch:[76/200] val cost:[3.915]\n",
      "epoch:[77/200] train cost:[4.129]\n",
      "epoch:[77/200] val cost:[3.938]\n",
      "epoch:[78/200] train cost:[4.106]\n",
      "epoch:[78/200] val cost:[3.905]\n",
      "epoch:[79/200] train cost:[4.105]\n",
      "epoch:[79/200] val cost:[3.902]\n",
      "epoch:[80/200] train cost:[4.083]\n",
      "epoch:[80/200] val cost:[3.892]\n",
      "epoch:[81/200] train cost:[4.071]\n",
      "epoch:[81/200] val cost:[3.897]\n",
      "epoch:[82/200] train cost:[4.061]\n",
      "epoch:[82/200] val cost:[3.883]\n",
      "epoch:[83/200] train cost:[4.048]\n",
      "epoch:[83/200] val cost:[3.873]\n",
      "epoch:[84/200] train cost:[4.029]\n",
      "epoch:[84/200] val cost:[3.854]\n",
      "epoch:[85/200] train cost:[4.023]\n",
      "epoch:[85/200] val cost:[3.854]\n",
      "epoch:[86/200] train cost:[4.011]\n",
      "epoch:[86/200] val cost:[3.853]\n",
      "epoch:[87/200] train cost:[3.999]\n",
      "epoch:[87/200] val cost:[3.833]\n",
      "epoch:[88/200] train cost:[3.980]\n",
      "epoch:[88/200] val cost:[3.840]\n",
      "epoch:[89/200] train cost:[3.973]\n",
      "epoch:[89/200] val cost:[3.827]\n",
      "epoch:[90/200] train cost:[3.960]\n",
      "epoch:[90/200] val cost:[3.829]\n",
      "epoch:[91/200] train cost:[3.951]\n",
      "epoch:[91/200] val cost:[3.805]\n",
      "epoch:[92/200] train cost:[3.942]\n",
      "epoch:[92/200] val cost:[3.808]\n",
      "epoch:[93/200] train cost:[3.932]\n",
      "epoch:[93/200] val cost:[3.811]\n",
      "epoch:[94/200] train cost:[3.918]\n",
      "epoch:[94/200] val cost:[3.792]\n",
      "epoch:[95/200] train cost:[3.911]\n",
      "epoch:[95/200] val cost:[3.791]\n",
      "epoch:[96/200] train cost:[3.899]\n",
      "epoch:[96/200] val cost:[3.789]\n",
      "epoch:[97/200] train cost:[3.882]\n",
      "epoch:[97/200] val cost:[3.785]\n",
      "epoch:[98/200] train cost:[3.872]\n",
      "epoch:[98/200] val cost:[3.786]\n",
      "epoch:[99/200] train cost:[3.865]\n",
      "epoch:[99/200] val cost:[3.762]\n",
      "epoch:[100/200] train cost:[3.853]\n",
      "epoch:[100/200] val cost:[3.754]\n",
      "epoch:[101/200] train cost:[3.845]\n",
      "epoch:[101/200] val cost:[3.775]\n",
      "epoch:[102/200] train cost:[3.834]\n",
      "epoch:[102/200] val cost:[3.766]\n",
      "epoch:[103/200] train cost:[3.825]\n",
      "epoch:[103/200] val cost:[3.742]\n",
      "epoch:[104/200] train cost:[3.813]\n",
      "epoch:[104/200] val cost:[3.738]\n",
      "epoch:[105/200] train cost:[3.806]\n",
      "epoch:[105/200] val cost:[3.736]\n",
      "epoch:[106/200] train cost:[3.795]\n",
      "epoch:[106/200] val cost:[3.742]\n",
      "epoch:[107/200] train cost:[3.790]\n",
      "epoch:[107/200] val cost:[3.734]\n",
      "epoch:[108/200] train cost:[3.778]\n",
      "epoch:[108/200] val cost:[3.741]\n",
      "epoch:[109/200] train cost:[3.769]\n",
      "epoch:[109/200] val cost:[3.724]\n",
      "epoch:[110/200] train cost:[3.756]\n",
      "epoch:[110/200] val cost:[3.717]\n",
      "epoch:[111/200] train cost:[3.752]\n",
      "epoch:[111/200] val cost:[3.702]\n",
      "epoch:[112/200] train cost:[3.748]\n",
      "epoch:[112/200] val cost:[3.700]\n",
      "epoch:[113/200] train cost:[3.736]\n",
      "epoch:[113/200] val cost:[3.709]\n",
      "epoch:[114/200] train cost:[3.725]\n",
      "epoch:[114/200] val cost:[3.703]\n",
      "epoch:[115/200] train cost:[3.720]\n",
      "epoch:[115/200] val cost:[3.687]\n",
      "epoch:[116/200] train cost:[3.709]\n",
      "epoch:[116/200] val cost:[3.697]\n",
      "epoch:[117/200] train cost:[3.702]\n",
      "epoch:[117/200] val cost:[3.684]\n",
      "epoch:[118/200] train cost:[3.695]\n",
      "epoch:[118/200] val cost:[3.683]\n",
      "epoch:[119/200] train cost:[3.686]\n",
      "epoch:[119/200] val cost:[3.675]\n",
      "epoch:[120/200] train cost:[3.678]\n",
      "epoch:[120/200] val cost:[3.678]\n",
      "epoch:[121/200] train cost:[3.668]\n",
      "epoch:[121/200] val cost:[3.679]\n",
      "epoch:[122/200] train cost:[3.664]\n",
      "epoch:[122/200] val cost:[3.663]\n",
      "epoch:[123/200] train cost:[3.653]\n",
      "epoch:[123/200] val cost:[3.680]\n",
      "epoch:[124/200] train cost:[3.648]\n",
      "epoch:[124/200] val cost:[3.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[125/200] train cost:[3.637]\n",
      "epoch:[125/200] val cost:[3.673]\n",
      "epoch:[126/200] train cost:[3.629]\n",
      "epoch:[126/200] val cost:[3.653]\n",
      "epoch:[127/200] train cost:[3.630]\n",
      "epoch:[127/200] val cost:[3.651]\n",
      "epoch:[128/200] train cost:[3.618]\n",
      "epoch:[128/200] val cost:[3.658]\n",
      "epoch:[129/200] train cost:[3.605]\n",
      "epoch:[129/200] val cost:[3.676]\n",
      "epoch:[130/200] train cost:[3.602]\n",
      "epoch:[130/200] val cost:[3.640]\n",
      "epoch:[131/200] train cost:[3.594]\n",
      "epoch:[131/200] val cost:[3.641]\n",
      "epoch:[132/200] train cost:[3.595]\n",
      "epoch:[132/200] val cost:[3.630]\n",
      "epoch:[133/200] train cost:[3.581]\n",
      "epoch:[133/200] val cost:[3.619]\n",
      "epoch:[134/200] train cost:[3.566]\n",
      "epoch:[134/200] val cost:[3.616]\n",
      "epoch:[135/200] train cost:[3.564]\n",
      "epoch:[135/200] val cost:[3.639]\n",
      "epoch:[136/200] train cost:[3.564]\n",
      "epoch:[136/200] val cost:[3.627]\n",
      "epoch:[137/200] train cost:[3.555]\n",
      "epoch:[137/200] val cost:[3.641]\n",
      "epoch:[138/200] train cost:[3.546]\n",
      "epoch:[138/200] val cost:[3.649]\n",
      "epoch:[139/200] train cost:[3.544]\n",
      "epoch:[139/200] val cost:[3.611]\n",
      "epoch:[140/200] train cost:[3.533]\n",
      "epoch:[140/200] val cost:[3.643]\n",
      "epoch:[141/200] train cost:[3.530]\n",
      "epoch:[141/200] val cost:[3.616]\n",
      "epoch:[142/200] train cost:[3.522]\n",
      "epoch:[142/200] val cost:[3.615]\n",
      "epoch:[143/200] train cost:[3.514]\n",
      "epoch:[143/200] val cost:[3.618]\n",
      "epoch:[144/200] train cost:[3.510]\n",
      "epoch:[144/200] val cost:[3.623]\n",
      "epoch:[145/200] train cost:[3.505]\n",
      "epoch:[145/200] val cost:[3.601]\n",
      "epoch:[146/200] train cost:[3.500]\n",
      "epoch:[146/200] val cost:[3.597]\n",
      "epoch:[147/200] train cost:[3.494]\n",
      "epoch:[147/200] val cost:[3.597]\n",
      "epoch:[148/200] train cost:[3.487]\n",
      "epoch:[148/200] val cost:[3.600]\n",
      "epoch:[149/200] train cost:[3.484]\n",
      "epoch:[149/200] val cost:[3.608]\n",
      "epoch:[150/200] train cost:[3.471]\n",
      "epoch:[150/200] val cost:[3.609]\n",
      "epoch:[151/200] train cost:[3.463]\n",
      "epoch:[151/200] val cost:[3.589]\n",
      "epoch:[152/200] train cost:[3.461]\n",
      "epoch:[152/200] val cost:[3.580]\n",
      "epoch:[153/200] train cost:[3.460]\n",
      "epoch:[153/200] val cost:[3.576]\n",
      "epoch:[154/200] train cost:[3.453]\n",
      "epoch:[154/200] val cost:[3.591]\n",
      "epoch:[155/200] train cost:[3.444]\n",
      "epoch:[155/200] val cost:[3.574]\n",
      "epoch:[156/200] train cost:[3.442]\n",
      "epoch:[156/200] val cost:[3.587]\n",
      "epoch:[157/200] train cost:[3.434]\n",
      "epoch:[157/200] val cost:[3.581]\n",
      "epoch:[158/200] train cost:[3.433]\n",
      "epoch:[158/200] val cost:[3.581]\n",
      "epoch:[159/200] train cost:[3.426]\n",
      "epoch:[159/200] val cost:[3.569]\n",
      "epoch:[160/200] train cost:[3.420]\n",
      "epoch:[160/200] val cost:[3.589]\n",
      "epoch:[161/200] train cost:[3.419]\n",
      "epoch:[161/200] val cost:[3.576]\n",
      "epoch:[162/200] train cost:[3.407]\n",
      "epoch:[162/200] val cost:[3.576]\n",
      "epoch:[163/200] train cost:[3.405]\n",
      "epoch:[163/200] val cost:[3.559]\n",
      "epoch:[164/200] train cost:[3.401]\n",
      "epoch:[164/200] val cost:[3.559]\n",
      "epoch:[165/200] train cost:[3.397]\n",
      "epoch:[165/200] val cost:[3.576]\n",
      "epoch:[166/200] train cost:[3.393]\n",
      "epoch:[166/200] val cost:[3.547]\n",
      "epoch:[167/200] train cost:[3.382]\n",
      "epoch:[167/200] val cost:[3.567]\n",
      "epoch:[168/200] train cost:[3.380]\n",
      "epoch:[168/200] val cost:[3.570]\n",
      "epoch:[169/200] train cost:[3.378]\n",
      "epoch:[169/200] val cost:[3.549]\n",
      "epoch:[170/200] train cost:[3.372]\n",
      "epoch:[170/200] val cost:[3.563]\n",
      "epoch:[171/200] train cost:[3.370]\n",
      "epoch:[171/200] val cost:[3.550]\n",
      "epoch:[172/200] train cost:[3.361]\n",
      "epoch:[172/200] val cost:[3.546]\n",
      "epoch:[173/200] train cost:[3.355]\n",
      "epoch:[173/200] val cost:[3.549]\n",
      "epoch:[174/200] train cost:[3.353]\n",
      "epoch:[174/200] val cost:[3.559]\n",
      "epoch:[175/200] train cost:[3.347]\n",
      "epoch:[175/200] val cost:[3.547]\n",
      "epoch:[176/200] train cost:[3.347]\n",
      "epoch:[176/200] val cost:[3.551]\n",
      "epoch:[177/200] train cost:[3.338]\n",
      "epoch:[177/200] val cost:[3.539]\n",
      "epoch:[178/200] train cost:[3.331]\n",
      "epoch:[178/200] val cost:[3.544]\n",
      "epoch:[179/200] train cost:[3.328]\n",
      "epoch:[179/200] val cost:[3.551]\n",
      "epoch:[180/200] train cost:[3.331]\n",
      "epoch:[180/200] val cost:[3.557]\n",
      "epoch:[181/200] train cost:[3.322]\n",
      "epoch:[181/200] val cost:[3.533]\n",
      "epoch:[182/200] train cost:[3.321]\n",
      "epoch:[182/200] val cost:[3.531]\n",
      "epoch:[183/200] train cost:[3.317]\n",
      "epoch:[183/200] val cost:[3.539]\n",
      "epoch:[184/200] train cost:[3.308]\n",
      "epoch:[184/200] val cost:[3.539]\n",
      "epoch:[185/200] train cost:[3.304]\n",
      "epoch:[185/200] val cost:[3.527]\n",
      "epoch:[186/200] train cost:[3.299]\n",
      "epoch:[186/200] val cost:[3.535]\n",
      "epoch:[187/200] train cost:[3.293]\n",
      "epoch:[187/200] val cost:[3.527]\n",
      "epoch:[188/200] train cost:[3.296]\n",
      "epoch:[188/200] val cost:[3.534]\n",
      "epoch:[189/200] train cost:[3.293]\n",
      "epoch:[189/200] val cost:[3.520]\n",
      "epoch:[190/200] train cost:[3.286]\n",
      "epoch:[190/200] val cost:[3.520]\n",
      "epoch:[191/200] train cost:[3.283]\n",
      "epoch:[191/200] val cost:[3.541]\n",
      "epoch:[192/200] train cost:[3.277]\n",
      "epoch:[192/200] val cost:[3.533]\n",
      "epoch:[193/200] train cost:[3.276]\n",
      "epoch:[193/200] val cost:[3.530]\n",
      "epoch:[194/200] train cost:[3.266]\n",
      "epoch:[194/200] val cost:[3.515]\n",
      "epoch:[195/200] train cost:[3.263]\n",
      "epoch:[195/200] val cost:[3.509]\n",
      "epoch:[196/200] train cost:[3.259]\n",
      "epoch:[196/200] val cost:[3.517]\n",
      "epoch:[197/200] train cost:[3.259]\n",
      "epoch:[197/200] val cost:[3.514]\n",
      "epoch:[198/200] train cost:[3.257]\n",
      "epoch:[198/200] val cost:[3.507]\n",
      "epoch:[199/200] train cost:[3.253]\n",
      "epoch:[199/200] val cost:[3.529]\n",
      "epoch:[200/200] train cost:[3.249]\n",
      "epoch:[200/200] val cost:[3.511]\n",
      "training done!!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-63fced0ae268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-add8d15e4cda>\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, optimizer, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m                \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                epoch: int):\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     state = {'epoch' : epoch, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "model = train(model, data_loaders, criterion, optimizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, optimizer, config.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_sentence(indeces, vocab):\n",
    "    res = []\n",
    "    eos = vocab.stoi['<eos>']\n",
    "    for i in indeces:\n",
    "        if i == eos:\n",
    "            res.append(vocab.itos[i])\n",
    "            break\n",
    "        else:\n",
    "            res.append(vocab.itos[i])\n",
    "    return ' '.join(res)\n",
    "    \n",
    "\n",
    "def test_model(model, data_loaders, SRC, TRG):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for batch in data_loaders['test']:\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "        predicted = model.predict(src)\n",
    "        \n",
    "        for ins in range(predicted.size(0)):\n",
    "            print(f'source : {index_to_sentence(src[ins],SRC.vocab)}')\n",
    "            print(f'target : {index_to_sentence(trg[ins], TRG.vocab)}')\n",
    "            print(f'predic : {index_to_sentence(predicted[ins], TRG.vocab)}')\n",
    "            print('*********************************************')\n",
    "\n",
    "                  \n",
    "        \n",
    "#             print([index_to_sentence(src[ins],SRC.vocab),\\\n",
    "#                    index_to_sentence(trg[ins], TRG.vocab),\\\n",
    "#                    index_to_sentence(predicted[ins], TRG.vocab)])\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source : <sos> ein cowboy <unk> seinen arm . <eos>\n",
      "target : <sos> a cowboy wrapping up his arm with a bandage . <eos>\n",
      "predic : <sos> a cowboy hat his arm . <eos>\n",
      "*********************************************\n",
      "source : <sos> der afroamerikaner protestiert gegen <unk> <unk> . <eos>\n",
      "target : <sos> the african american man <unk> against <unk> <unk> . <eos>\n",
      "predic : <sos> the african american man is protesting against pollution . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein afroamerikaner geht die straße hinunter . <eos>\n",
      "target : <sos> an african american man walking down the street . <eos>\n",
      "predic : <sos> an african american man walking down the street . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein junger mann wirft einen football . <eos>\n",
      "target : <sos> a young man about to throw a football . <eos>\n",
      "predic : <sos> a young man throws a football ball . <eos>\n",
      "*********************************************\n",
      "source : <sos> eine gruppe macht tricks auf motorrädern . <eos>\n",
      "target : <sos> a group of people do tricks on motorbikes . <eos>\n",
      "predic : <sos> a group of people doing tricks on bikes . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein brauner hund gräbt im dreck . <eos>\n",
      "target : <sos> a brown dog is digging in the dirt . <eos>\n",
      "predic : <sos> a brown dog digging in the dirt . <eos>\n",
      "*********************************************\n",
      "source : <sos> kleine kinder fahren in einem <unk> . <eos>\n",
      "target : <sos> young kids are on a small train ride . <eos>\n",
      "predic : <sos> small children are riding in a <unk> . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein schwarzer junge sitzt im sand . <eos>\n",
      "target : <sos> a black boy is sitting in the sand . <eos>\n",
      "predic : <sos> a black boy sits in the sand . <eos>\n",
      "*********************************************\n",
      "source : <sos> geparkte autos mit einem schulbus dahinter . <eos>\n",
      "target : <sos> parked cars with a school bus behind them . <eos>\n",
      "predic : <sos> cars parked with a school bus behind them . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei braune hunde spielen grob miteinander . <eos>\n",
      "target : <sos> two brown dogs playing in a rough manner . <eos>\n",
      "predic : <sos> two brown dogs play with each other . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein braun-weißer hund holt ein spielzeug . <eos>\n",
      "target : <sos> a brown and white dog fetching a toy . <eos>\n",
      "predic : <sos> a brown and white dog fetching a toy . <eos>\n",
      "*********************************************\n",
      "source : <sos> eine große menschenmenge füllt eine straße . <eos>\n",
      "target : <sos> a large group of people fill a street . <eos>\n",
      "predic : <sos> a large crowd of people filling a street . <eos>\n",
      "*********************************************\n",
      "source : <sos> leute reparieren das dach eines hauses . <eos>\n",
      "target : <sos> people are fixing the roof of a house . <eos>\n",
      "predic : <sos> people repair the roof of a house . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei autos fahren auf einer rennstrecke . <eos>\n",
      "target : <sos> two cars are driving on a racetrack . <eos>\n",
      "predic : <sos> two cars are riding on a racetrack . <eos>\n",
      "*********************************************\n",
      "source : <sos> leute spielen ein spiel im pool . <eos>\n",
      "target : <sos> people playing a game in the pool . <eos>\n",
      "predic : <sos> people playing a game in the pool . <eos>\n",
      "*********************************************\n",
      "source : <sos> drei männer kochen in einer küche . <eos>\n",
      "target : <sos> three men are cooking in a kitchen . <eos>\n",
      "predic : <sos> three men cooking in a kitchen . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei kinder spielen auf einem fahrrad . <eos>\n",
      "target : <sos> two children are playing on a bicycle . <eos>\n",
      "predic : <sos> two children play on a bicycle . <eos>\n",
      "*********************************************\n",
      "source : <sos> <unk> , die alle fahrrad fahren . <eos>\n",
      "target : <sos> crowds of people are all riding bicycles . <eos>\n",
      "predic : <sos> <unk> <unk> animals somewhere . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei jungen spielen auf dem gehsteig . <eos>\n",
      "target : <sos> two boys are playing on the sidewalk . <eos>\n",
      "predic : <sos> two boys play on the sidewalk . <eos>\n",
      "*********************************************\n",
      "source : <sos> drei mädchen lächeln für ein foto . <eos>\n",
      "target : <sos> three girls are smiling for a picture . <eos>\n",
      "predic : <sos> three girls smile for a picture . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei <unk> posieren für ein foto . <eos>\n",
      "target : <sos> two <unk> are posing for a picture . <eos>\n",
      "predic : <sos> two <unk> pose for a picture . <eos>\n",
      "*********************************************\n",
      "source : <sos> bauarbeiter <unk> gegen <unk> <unk> <unk> . <eos>\n",
      "target : <sos> construction workers <unk> against <unk> construction services . <eos>\n",
      "predic : <sos> construction workers are installing <unk> <unk> <unk> <unk> . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein paar pflanzen wachsen beim fenster . <eos>\n",
      "target : <sos> some plants are growing near the window . <eos>\n",
      "predic : <sos> a couple planting plants growing on the window . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei typen mit <unk> <unk> lächeln . <eos>\n",
      "target : <sos> two guys with <unk> piercings are smiling . <eos>\n",
      "predic : <sos> two guys in <unk> caps are smiling . <eos>\n",
      "*********************************************\n",
      "source : <sos> eine gruppe ist in einer bar . <eos>\n",
      "target : <sos> a crowd is present at a bar . <eos>\n",
      "predic : <sos> a group of people are in a bar . <eos>\n",
      "*********************************************\n",
      "source : <sos> eine asiatin steckt ihre haare zurück . <eos>\n",
      "target : <sos> an asian woman <unk> her hair back . <eos>\n",
      "predic : <sos> an asian woman is sticking her hair out her hair . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei deutsche schäferhunde <unk> einander an . <eos>\n",
      "target : <sos> two german <unk> snarling at each other . <eos>\n",
      "predic : <sos> two german shepherd dogs are <unk> to each other . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein <unk> springt nach einem ball . <eos>\n",
      "target : <sos> a <unk> terrier leaps after a ball . <eos>\n",
      "predic : <sos> a <unk> is jumping for a ball . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein mann arbeitet an einem <unk> . <eos>\n",
      "target : <sos> a man is working a hotdog stand . <eos>\n",
      "predic : <sos> a man works on a <unk> chair . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei jungen vor einem <unk> . <eos>\n",
      "target : <sos> two boys in front of a soda machine . <eos>\n",
      "predic : <sos> two boys in front of a <unk> <unk> . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein typ küsst einen anderen typ <eos>\n",
      "target : <sos> a guy give a kiss to a guy also <eos>\n",
      "predic : <sos> a guy kissing another guy 's face . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein wandgemälde auf einem gebäude . <eos>\n",
      "target : <sos> a mural on the side of a building . <eos>\n",
      "predic : <sos> a mural on a building . <eos>\n",
      "*********************************************\n",
      "source : <sos> arbeiter diskutieren neben den schienen . <eos>\n",
      "target : <sos> construction workers having a discussion by the tracks . <eos>\n",
      "predic : <sos> workers discussing the guard guard next to the railroad . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei jungen spielen gegeneinander fußball . <eos>\n",
      "target : <sos> two boys play soccer against each other . <eos>\n",
      "predic : <sos> two boys are playing soccer ball . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei fußballmannschaften auf dem feld . <eos>\n",
      "target : <sos> two soccer teams are on the field . <eos>\n",
      "predic : <sos> two soccer players on the field . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein hellbrauner hund läuft bergauf . <eos>\n",
      "target : <sos> a light brown dog is running up . <eos>\n",
      "predic : <sos> a tan dog is running uphill . <eos>\n",
      "*********************************************\n",
      "source : <sos> leute bewundern ein kunstwerk . <eos>\n",
      "target : <sos> people are admiring a work of art . <eos>\n",
      "predic : <sos> people admire art art . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein junge steht mit drei mädchen . <eos>\n",
      "target : <sos> a boy stands with three girls . <eos>\n",
      "predic : <sos> a boy stands with three girls . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei männer sitzen in einem restaurant . <eos>\n",
      "target : <sos> two men sitting in a restaurant . <eos>\n",
      "predic : <sos> two men are sitting in a restaurant . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei typen und eine frau lächeln . <eos>\n",
      "target : <sos> two guys and a girl smiling . <eos>\n",
      "predic : <sos> two guys and a woman smile . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei kleine kinder auf dem sand . <eos>\n",
      "target : <sos> two young children are on sand . <eos>\n",
      "predic : <sos> two small children on the sand . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein kleiner schwarzer hund springt über <unk> <eos>\n",
      "target : <sos> a small black dog jumping over gates <eos>\n",
      "predic : <sos> a little black dog jumps over the <unk> of <unk> . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei hunde spielen an einem baum . <eos>\n",
      "target : <sos> two dogs play by a tree . <eos>\n",
      "predic : <sos> two dogs play on a tree . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein junges mädchen schwimmt in einem pool <eos>\n",
      "target : <sos> a young girl swimming in a pool <eos>\n",
      "predic : <sos> a young girl swims in a pool . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei männer in schwarz in einer stadt <eos>\n",
      "target : <sos> two men wearing black in a city <eos>\n",
      "predic : <sos> two men in black in a city . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein mann schneidet äste von bäumen . <eos>\n",
      "target : <sos> a man cutting branches of trees . <eos>\n",
      "predic : <sos> a man is cutting some leafless trees . <eos>\n",
      "*********************************************\n",
      "source : <sos> drei leute sitzen in einer höhle . <eos>\n",
      "target : <sos> three people sit in a cave . <eos>\n",
      "predic : <sos> three people are sitting in a cave . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein typ arbeitet an einem gebäude . <eos>\n",
      "target : <sos> a guy works on a building . <eos>\n",
      "predic : <sos> a guy working on a building . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein reh springt über einen zaun . <eos>\n",
      "target : <sos> a deer jumps a fence . <eos>\n",
      "predic : <sos> a deer jumps over a fence . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein radfahrer springt über eine hindernis . <eos>\n",
      "target : <sos> a biker jumps an obstacle . <eos>\n",
      "predic : <sos> a cyclist is jumping over a hurdle . <eos>\n",
      "*********************************************\n",
      "source : <sos> ärzte bei einer art operation . <eos>\n",
      "target : <sos> doctors performing some type of surgery . <eos>\n",
      "predic : <sos> doctors are attending a conference event . <eos>\n",
      "*********************************************\n",
      "source : <sos> feuerwehrmänner kommen aus einer u-bahnstation . <eos>\n",
      "target : <sos> firemen <unk> from a subway station . <eos>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predic : <sos> firefighters are coming out of a subway . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein mann verwendet <unk> geräte . <eos>\n",
      "target : <sos> a man is using electronic equipment . <eos>\n",
      "predic : <sos> a man uses cupcakes . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein mann an seinem hochzeitstag . <eos>\n",
      "target : <sos> a man on his wedding day . <eos>\n",
      "predic : <sos> a man on his wedding day . <eos>\n",
      "*********************************************\n",
      "source : <sos> hunde laufen auf einer hunderennbahn . <eos>\n",
      "target : <sos> dogs run at a dog racetrack . <eos>\n",
      "predic : <sos> dogs are running on a dog . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein am strand <unk> auto . <eos>\n",
      "target : <sos> a car parked at the beach . <eos>\n",
      "predic : <sos> a <unk> man on the beach . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein kind planscht im wasser . <eos>\n",
      "target : <sos> a child is splashing in the water <eos>\n",
      "predic : <sos> a child splashing in the water . <eos>\n",
      "*********************************************\n",
      "source : <sos> leute sitzen in einem zug . <eos>\n",
      "target : <sos> people sit inside a train . <eos>\n",
      "predic : <sos> people sitting in a train . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein rockkonzert findet statt . <eos>\n",
      "target : <sos> a rock concert is taking place . <eos>\n",
      "predic : <sos> a band is taking place . <eos>\n",
      "*********************************************\n",
      "source : <sos> ein <unk> inspiziert etwas . <eos>\n",
      "target : <sos> an army officer is inspecting something . <eos>\n",
      "predic : <sos> a <unk> is inspecting something . <eos>\n",
      "*********************************************\n",
      "source : <sos> drei männer gehen bergauf . <eos>\n",
      "target : <sos> three men are walking up hill . <eos>\n",
      "predic : <sos> three men are walking in the snow . <eos>\n",
      "*********************************************\n",
      "source : <sos> eine frau spielt volleyball . <eos>\n",
      "target : <sos> a woman is playing volleyball . <eos>\n",
      "predic : <sos> a woman plays volleyball . <eos>\n",
      "*********************************************\n",
      "source : <sos> junge frau klettert auf felswand <eos>\n",
      "target : <sos> young woman climbing rock face <eos>\n",
      "predic : <sos> young woman climbing on a rock wall . <eos>\n",
      "*********************************************\n",
      "source : <sos> zwei männer mit mützen . <eos>\n",
      "target : <sos> two men wearing hats . <eos>\n",
      "predic : <sos> two men wearing hats are wearing hats . <eos>\n",
      "*********************************************\n"
     ]
    }
   ],
   "source": [
    "test_model(model, data_loaders, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 4,  ..., 4, 4, 4],\n",
       "        [2, 4, 4,  ..., 4, 4, 4],\n",
       "        [2, 4, 4,  ..., 4, 4, 4],\n",
       "        ...,\n",
       "        [2, 4, 4,  ..., 4, 4, 4],\n",
       "        [2, 4, 4,  ..., 4, 4, 4],\n",
       "        [2, 4, 4,  ..., 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "for batch in data_loaders['test']:\n",
    "    \n",
    "#     src = batch.src[0,:].unsqueeze(0).to(device)\n",
    "    src = batch.src.to(device)\n",
    "#     trg = batch.trg.to(device)\n",
    "    \n",
    "    p = model.predict(src)\n",
    "    break\n",
    "# batch.src[0,:].unsqueeze(0)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   4, 429,  ...,   3,   5,   3],\n",
       "        [  2,   7, 324,  ...,   5,   3,   5],\n",
       "        [  2,  21, 324,  ...,   5,   3,   5],\n",
       "        ...,\n",
       "        [  2,   4,  14,  ...,   5,   3,   5],\n",
       "        [  2,  24,  14,  ...,   3,   5,   3],\n",
       "        [  2,  16,  30,  ...,   3,   5,   3]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "for batch in data_loaders['test']:\n",
    "    \n",
    "#     src = batch.src[0,:].unsqueeze(0).to(device)\n",
    "    src = batch.src.to(device)\n",
    "#     trg = batch.trg.to(device)\n",
    "    \n",
    "    p = model.predict(src)\n",
    "    break\n",
    "# batch.src[0,:].unsqueeze(0)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   4, 429,  67,  27, 394,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,\n",
       "          5,   3,   5,   3], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = p[0]\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "cowboy\n",
      "hat\n",
      "his\n",
      "arm\n",
      ".\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.itos[4])\n",
    "print(TRG.vocab.itos[429])\n",
    "print(TRG.vocab.itos[67])\n",
    "print(TRG.vocab.itos[27])\n",
    "print(TRG.vocab.itos[394])\n",
    "print(TRG.vocab.itos[5])\n",
    "print(TRG.vocab.itos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_embedding.weight \t tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "dec_embedding.weight \t tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "encoder.embedding_table.weight \t tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor])\n",
    "    i+=1\n",
    "    if i == 3:\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, criterion2, optimizer2 = get_network(SRC, TRG, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_embedding.weight \t tensor([[ 0.2050, -0.1782, -0.3098,  ..., -1.4686, -0.3315, -1.2724],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4540, -1.2224, -0.4631,  ..., -1.2043, -1.1045, -0.4408],\n",
      "        ...,\n",
      "        [ 0.1277,  0.1854,  0.1556,  ..., -0.7096,  0.1593, -1.2922],\n",
      "        [-0.9370, -1.2006,  0.4956,  ..., -1.7500, -0.4906, -1.7067],\n",
      "        [-0.6994,  0.7447, -0.7396,  ..., -1.4942, -0.8825,  0.4950]],\n",
      "       device='cuda:0')\n",
      "dec_embedding.weight \t tensor([[ 0.5049, -0.4566,  0.4133,  ...,  2.3915, -0.3424, -0.3108],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1504,  1.7340, -0.0179,  ..., -1.4123, -0.4329, -1.1875],\n",
      "        ...,\n",
      "        [ 1.1088, -3.1266, -0.5490,  ..., -1.1389, -0.1900,  1.8774],\n",
      "        [ 0.6562, -0.1499,  1.0171,  ...,  1.3236,  0.1412,  0.5640],\n",
      "        [ 1.1741,  0.7020,  0.0589,  ..., -1.0486,  0.4446, -0.2674]],\n",
      "       device='cuda:0')\n",
      "encoder.embedding_table.weight \t tensor([[ 0.2050, -0.1782, -0.3098,  ..., -1.4686, -0.3315, -1.2724],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4540, -1.2224, -0.4631,  ..., -1.2043, -1.1045, -0.4408],\n",
      "        ...,\n",
      "        [ 0.1277,  0.1854,  0.1556,  ..., -0.7096,  0.1593, -1.2922],\n",
      "        [-0.9370, -1.2006,  0.4956,  ..., -1.7500, -0.4906, -1.7067],\n",
      "        [-0.6994,  0.7447, -0.7396,  ..., -1.4942, -0.8825,  0.4950]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor])\n",
    "    i+=1\n",
    "    if i == 3:\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 02 17:17:25 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 166... WDDM  | 00000000:26:00.0  On |                  N/A |\n",
      "| 30%   38C    P8    11W / 125W |   5367MiB /  6144MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4328    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      5852    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6668    C+G   ...4__8wekyb3d8bbwe\\Time.exe    N/A      |\n",
      "|    0   N/A  N/A      6952    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7504    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
      "|    0   N/A  N/A      7576    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      8160    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9092    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      9452      C   D:\\anaconda3\\python.exe         N/A      |\n",
      "|    0   N/A  N/A     13296    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     14720    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     15284    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16336    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:09<15:25,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.604 | Train PPL: 5453.463\n",
      "\t Val. Loss: 8.524 |  Val. PPL: 5035.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                                | 2/100 [00:18<15:16,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.451 | Train PPL: 4681.116\n",
      "\t Val. Loss: 8.371 |  Val. PPL: 4321.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                               | 3/100 [00:27<15:05,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.302 | Train PPL: 4033.552\n",
      "\t Val. Loss: 8.222 |  Val. PPL: 3722.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                              | 4/100 [00:37<14:54,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 8.157 | Train PPL: 3488.838\n",
      "\t Val. Loss: 8.077 |  Val. PPL: 3218.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                              | 4/100 [00:44<17:53, 11.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f97c9758a1b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-f97c9758a1b8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, enc_attention_scores, _ = model(src, trg)\n",
    "\n",
    "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, attention_score, _ = model(src, trg) #turn off teacher forcing\n",
    "\n",
    "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in tqdm(range(config.n_epochs), total=config.n_epochs):\n",
    "    train_loss = train(model, data_loders['train'], optimizer, criterion, config.gradient_clip)\n",
    "    valid_loss = evaluate(model, data_loders['val'], criterion)\n",
    "    \n",
    "#     if best_valid_loss < valid_loss:\n",
    "#         break\n",
    "#     else:\n",
    "#         best_valid_loss = valid_loss\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, data_loders['test'], criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_embedding.weight \t torch.Size([7854, 64])\n",
      "dec_embedding.weight \t torch.Size([5893, 64])\n",
      "encoder.embedding_table.weight \t torch.Size([7854, 64])\n",
      "encoder.embed_positions.weight \t torch.Size([512, 64])\n",
      "encoder.layers.0.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.0.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.0.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.0.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.0.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.0.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.1.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.1.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.1.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.1.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.1.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.1.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.2.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.self_attn.output.weight \t torch.Size([64, 64])\n",
      "encoder.layers.2.attn_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.2.attn_layer_norm.bias \t torch.Size([64])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "encoder.layers.2.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "encoder.layers.2.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "encoder.layers.2.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.embedding_table.weight \t torch.Size([5893, 64])\n",
      "decoder.embed_positions.weight \t torch.Size([512, 64])\n",
      "decoder.layers.0.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.0.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.0.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.0.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.0.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.0.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.1.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.1.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.1.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.1.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.self_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.self_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.self_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.enc_dec_attn.wk.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.wq.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.wv.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn.output.weight \t torch.Size([64, 64])\n",
      "decoder.layers.2.enc_dec_attn_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.enc_dec_attn_layer_norm.bias \t torch.Size([64])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_1.weight \t torch.Size([256, 64])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_1.bias \t torch.Size([256])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_2.weight \t torch.Size([64, 256])\n",
      "decoder.layers.2.position_wise_feed_forward.linear_2.bias \t torch.Size([64])\n",
      "decoder.layers.2.feed_forward_layer_norm.weight \t torch.Size([64])\n",
      "decoder.layers.2.feed_forward_layer_norm.bias \t torch.Size([64])\n",
      "linear.weight \t torch.Size([5893, 64])\n",
      "linear.bias \t torch.Size([5893])\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param_tensor in model.state_dict():\n",
    "    i+= 1\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             data_loders: dict,\n",
    "             criterion,\n",
    "             optimizer,\n",
    "             config):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
